# -*- coding: utf-8 -*-

# ==============================================================================
# === 0. Импорты и Установка ===================================================
# ==============================================================================
print("Импорт библиотек...")
!pip install --upgrade tensorflow tensorflow-tpu

!pip install datasets numpy pickle5

import tensorflow as tf
import numpy as np
import re
import time
import os
import pickle
from datasets import load_dataset
import shutil
import traceback
from collections import deque # <--- ДОБАВЛЕН ИМПОРТ

print(f"TensorFlow version: {tf.__version__}")
try:
    import datasets
    print(f"Datasets version: {datasets.__version__}")
except ImportError:
    print("Библиотека datasets не установлена.")
    raise RuntimeError("Библиотека datasets не установлена.")
    
print("Доступные физические устройства:", tf.config.list_physical_devices())
# Ожидаемый вывод должен включать 'TPU'

# ==============================================================================
# === 0.4 Установка Политики Точности (float32) ================================
# ==============================================================================
print("Установка глобальной политики Keras на float32...")
tf.keras.mixed_precision.set_global_policy('float32')
print(f"Текущая политика точности Keras: {tf.keras.mixed_precision.global_policy().name}")

# ==============================================================================
# === 0.5 Инициализация TPU и Стратегии ========================================
# ==============================================================================
print("\nИнициализация TPU и стратегии...")
strategy = None
try:
    resolver = tf.distribute.cluster_resolver.TPUClusterResolver('local')
    print(f"Подключение к TPU: {resolver.master()}")
    tf.config.experimental_connect_to_cluster(resolver)
    tf.tpu.experimental.initialize_tpu_system(resolver)
    strategy = tf.distribute.TPUStrategy(resolver)
    print("✅ TPU инициализирована успешно.")
    print('Количество реплик (TPU ядер): {}'.format(strategy.num_replicas_in_sync))
except (ValueError, RuntimeError) as e:
    print(f"❌ Ошибка инициализации TPU: {e}. Попытка использовать CPU/GPU...")
    strategy = tf.distribute.get_strategy()
    if isinstance(strategy, tf.distribute.MirroredStrategy):
        print(f"Используется MirroredStrategy с {strategy.num_replicas_in_sync} GPU.")
    elif tf.config.list_physical_devices('GPU'):
        print("Используется одна GPU (Default Strategy).")
    else:
        print("Используется CPU (Default Strategy).")

if strategy is None:
    raise RuntimeError("Не удалось определить стратегию выполнения.")

print(f"✅ Используемая стратегия: {strategy.__class__.__name__}")
print(f"✅ Количество реплик в синхронизации: {strategy.num_replicas_in_sync}")



# ==============================================================================
# === 1. Параметры (Модель ~46M, Обучение кастомное) ==========================
# ==============================================================================
print("\nУстановка параметров...")
DATASET_NAME = "daily_dialog"
DATA_DIR = "chatbot_data_simple_loop_std_mha"
TOKENIZER_FILENAME = "tokenizer_simple_loop_std_mha.pickle"
MODEL_FILENAME_BASE = "model_70M_simple_loop_std_mha"
CHECKPOINT_DIR = os.path.join(DATA_DIR, "checkpoints_" + MODEL_FILENAME_BASE)
FINAL_MODEL_PATH = os.path.join(DATA_DIR, MODEL_FILENAME_BASE + ".keras")

NUM_EXAMPLES = 120000; MAX_LENGTH = 40; BUFFER_SIZE = NUM_EXAMPLES; VOCAB_SIZE_LIMIT = 40000
NUM_LAYERS = 6; D_MODEL = 384; NUM_HEADS = 8; DFF = D_MODEL * 4; DROPOUT_RATE = 0.1
BATCH_SIZE_PER_REPLICA = 256
GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync
EPOCHS = 10
WARMUP_STEPS = 500
CHECKPOINT_SAVE_FREQ = 5

# --- Чат ---
DEFAULT_TEMPERATURE = 0.7
DEFAULT_REPETITION_PENALTY = 1.2
CONTEXT_WINDOW_SIZE = 10 # <--- РАЗМЕР ОКНА ИСТОРИИ (можно менять)

# --- Токены ---
PAD_TOKEN_ID = 0; START_TOKEN_ID = None; END_TOKEN_ID = None; VOCAB_SIZE = None

os.makedirs(DATA_DIR, exist_ok=True); os.makedirs(CHECKPOINT_DIR, exist_ok=True)
print(f"Model Params: D_MODEL={D_MODEL}, DFF={DFF}")
print(f"Training Params: BATCH_SIZE_PER_REPLICA={BATCH_SIZE_PER_REPLICA}, GLOBAL_BATCH_SIZE={GLOBAL_BATCH_SIZE}, EPOCHS={EPOCHS}, WARMUP_STEPS={WARMUP_STEPS}")
print(f"Chat Params: CONTEXT_WINDOW_SIZE={CONTEXT_WINDOW_SIZE}") # Добавлен вывод
print(f"Checkpoint directory: {CHECKPOINT_DIR}")


# ==============================================================================
# === 2. Функции Предобработки Данных (Адаптировано) ==========================
# ==============================================================================
def preprocess_sentence(w):
    """Препроцессит ОДНУ строку, добавляя <start>/<end>."""
    if not isinstance(w, str): w = str(w); w = w.lower().strip()
    w = re.sub(r"([?.!,])", r" \1 ", w); w = re.sub(r'[" "]+', " ", w)
    w = w.strip(); w = '<start> ' + w + ' <end>'; return w

# --- НОВАЯ ФУНКЦИЯ для контекста ---
def create_context_string(history_deque, current_input):
    """
    Создает единую строку из истории и текущего ввода для подачи в модель.
    ВАЖНО: Не применяет preprocess_sentence к отдельным частям истории.
    """
    # Собираем историю и текущий ввод
    full_input_list = list(history_deque) + [current_input]
    # Объединяем через специальный разделитель, который модель могла видеть
    # Или просто через пробел, если модель должна сама разобраться
    # Попробуем простой пробел
    context_string = " ".join(full_input_list)
    # print(f"[DEBUG] Context String before preprocess: '{context_string}'") # Для отладки
    return context_string
# --- КОНЕЦ НОВОЙ ФУНКЦИИ ---

def create_pairs_from_hf_dataset(dataset, num_examples, max_length):
    # ... (код без изменений, использует preprocess_sentence для пар) ...
    inputs, outputs = [], []; processed_dialogs = 0; print("Извлечение пар вопрос-ответ из датасета..."); total_dialogs = len(dataset)
    skipped_count = 0; stop_processing = False
    for example in dataset: # ... (логика цикла) ...
        if stop_processing: break
        processed_dialogs += 1; dialog = example.get('dialog', [])
        if isinstance(dialog, list) and len(dialog) > 1:
            for i in range(len(dialog) - 1): # ... (логика цикла) ...
                inp_raw = dialog[i]; out_raw = dialog[i+1]
                if not inp_raw or not out_raw or not isinstance(inp_raw, str) or not isinstance(out_raw, str): skipped_count += 1; continue
                inp = preprocess_sentence(inp_raw); out = preprocess_sentence(out_raw) # Препроцессим пару
                inp_len = len(inp.split()); out_len = len(out.split())
                if inp_len > 2 and inp_len <= max_length and out_len > 2 and out_len <= max_length: # ... (добавление) ...
                    inputs.append(inp); outputs.append(out)
                    if len(inputs) >= num_examples: print(f"\nДостигнут лимит в {num_examples} пар."); stop_processing = True; break
                else: skipped_count += 1
        else: skipped_count += 1
        # ... (логирование прогресса) ...
        if stop_processing: break
        if processed_dialogs % 1000 == 0 or processed_dialogs == total_dialogs: print(f"Обработано диалогов: {processed_dialogs}/{total_dialogs}, Собрано пар: {len(inputs)}/{num_examples}, Пропущено: {skipped_count}", end='\r')
    print(f"\nОбработка диалогов завершена. Сгенерировано {len(inputs)} пар.");
    if skipped_count > 0: print(f"(Пропущено неподходящих: {skipped_count})");
    return inputs, outputs

def load_and_prepare_data(dataset_name, num_examples, max_length, global_batch_size, buffer_size, vocab_limit, data_dir, tokenizer_filename):
    # ... (код без изменений, возвращает dataset ((enc_in, dec_in), dec_out)) ...
    print("\n--- Загрузка и обработка данных (формат для простого цикла) ---")
    tokenizer_path = os.path.join(data_dir, tokenizer_filename); tokenizer = None; all_questions, all_answers = [], []
    if os.path.exists(tokenizer_path): # ... (загрузка токенизатора) ...
        print(f"Загрузка токенизатора из {tokenizer_path}...");
        try:
            with open(tokenizer_path, 'rb') as handle: tokenizer = pickle.load(handle); print("Токенизатор загружен.")
        except Exception as e: print(f"Ошибка загрузки токенизатора: {e}. Создаем новый.")
    else: print("Токенизатор не найден. Создаем новый.")
    if tokenizer is None: # ... (создание токенизатора) ...
        try:
            print(f"Загрузка '{dataset_name}' для обучения токенизатора..."); raw_datasets = load_dataset(dataset_name, trust_remote_code=True)
            all_dialogs_for_tokenizer = [];
            for split_name in raw_datasets.keys(): print(f"Используем диалоги из '{split_name}' для словаря..."); all_dialogs_for_tokenizer.extend(raw_datasets[split_name])
            q_tok, a_tok = create_pairs_from_hf_dataset(all_dialogs_for_tokenizer, float('inf'), max_length)
            if not q_tok: raise ValueError("Не удалось извлечь пары для обучения токенизатора.")
            print("Создание и сохранение нового токенизатора..."); tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_limit, oov_token='<unk>', filters='')
            tokenizer.fit_on_texts(q_tok + a_tok)
            try:
                os.makedirs(data_dir, exist_ok=True);
                with open(tokenizer_path, 'wb') as handle: pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
                print(f"Токенизатор сохранен в {tokenizer_path}")
            except Exception as e: print(f"Ошибка сохранения токенизатора: {e}")
        except Exception as e: print(f"Ошибка при загрузке данных / создании токенизатора: {e}"); traceback.print_exc(); return None, None, None, None
    actual_vocab_size = len(tokenizer.word_index) + 1; vocab_size = min(vocab_limit, actual_vocab_size - 1) + 1 if vocab_limit else actual_vocab_size
    print(f"Итоговый размер словаря (с OOV и паддингом 0): {vocab_size}")
    start_token_id = tokenizer.word_index.get('<start>'); end_token_id = tokenizer.word_index.get('<end>')
    if start_token_id is None or end_token_id is None: print("Критическая ошибка: не найдены <start>/<end> токены!"); return None, tokenizer, vocab_size, None
    try: # ... (загрузка данных для датасета) ...
        print(f"Загрузка '{dataset_name}' для создания обучающего датасета..."); raw_datasets = load_dataset(dataset_name, trust_remote_code=True)
        if 'train' not in raw_datasets: raise ValueError("Сплит 'train' не найден.")
        train_dialogs = list(raw_datasets['train'])
        print(f"Извлечение пар для обучения (лимит {num_examples})..."); all_questions, all_answers = create_pairs_from_hf_dataset(train_dialogs, num_examples, max_length)
        if not all_questions: raise ValueError("Не удалось извлечь пары для обучения.")
    except Exception as e: print(f"Ошибка при подготовке данных для датасета: {e}"); traceback.print_exc(); return None, tokenizer, vocab_size, (start_token_id, end_token_id)
    print("Создание обучающего tf.data.Dataset...") # ... (создание датасета) ...
    question_seqs = tokenizer.texts_to_sequences(all_questions); answer_seqs = tokenizer.texts_to_sequences(all_answers)
    question_seqs = tf.keras.preprocessing.sequence.pad_sequences(question_seqs, maxlen=max_length, padding='post', truncating='post')
    answer_seqs = tf.keras.preprocessing.sequence.pad_sequences(answer_seqs, maxlen=max_length, padding='post', truncating='post')
    encoder_inputs = tf.cast(question_seqs, tf.int32); decoder_inputs = tf.cast(answer_seqs[:, :-1], tf.int32); decoder_outputs = tf.cast(answer_seqs[:, 1:], tf.int32)
    dataset = tf.data.Dataset.from_tensor_slices(((encoder_inputs, decoder_inputs), decoder_outputs))
    dataset = dataset.cache().shuffle(buffer_size).batch(global_batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)
    print(f"Обучающий датасет создан (батчей: ~{len(all_questions)//global_batch_size}).")
    print("\nПример обработанных данных из ОБУЧАЮЩЕГО набора:") # ... (вывод примера) ...
    for (enc_in, dec_in), dec_out in dataset.take(1):
        print(f" Вход Энкодера (форма): {enc_in.shape}"); print(f" Вход Декодера (форма): {dec_in.shape}"); print(f" Выход Декодера (форма): {dec_out.shape}")
        def _decode(seq):
             if hasattr(tokenizer, 'index_word'): return " ".join([tokenizer.index_word.get(int(i), '?') for i in seq if int(i) != 0])
             else: return "[Ошибка]"
        print(f"  Encoder Input Text: {_decode(enc_in[0].numpy())}"); print(f"  Decoder Input Text: {_decode(dec_in[0].numpy())}"); print(f"  Decoder Target Text: {_decode(dec_out[0].numpy())}")
        break
    return dataset, tokenizer, vocab_size, (start_token_id, end_token_id)

# --- Запуск подготовки данных ---
# ... (код без изменений) ...
print("Запуск load_and_prepare_data...")
train_dataset, tokenizer, VOCAB_SIZE, token_ids = load_and_prepare_data(
    dataset_name=DATASET_NAME, num_examples=NUM_EXAMPLES, max_length=MAX_LENGTH,
    global_batch_size=GLOBAL_BATCH_SIZE, buffer_size=BUFFER_SIZE, vocab_limit=VOCAB_SIZE_LIMIT,
    data_dir=DATA_DIR, tokenizer_filename=TOKENIZER_FILENAME
)
if train_dataset is None or tokenizer is None or VOCAB_SIZE is None or token_ids is None: raise RuntimeError("Ошибка во время подготовки данных.")
else:
    START_TOKEN_ID, END_TOKEN_ID = token_ids; input_vocab_size = target_vocab_size = VOCAB_SIZE
    train_dataset_dist = strategy.experimental_distribute_dataset(train_dataset)
    print("\nПодготовка данных успешно завершена."); print(f"Итоговый размер словаря: {VOCAB_SIZE}")
    print(f"ID <start>: {START_TOKEN_ID}, ID <end>: {END_TOKEN_ID}, ID <pad>: {PAD_TOKEN_ID}")
    print(f"Датасет распределен для {strategy.__class__.__name__}."); print("-" * 30 + "\n")

# ==============================================================================
# === 4. Архитектура Модели Transformer (СТАНДАРТНЫЙ MHA) =====================
# ==============================================================================
print("--- Определение Архитектуры Модели (СТАНДАРТНЫЙ Keras MHA) ---")
# --- Копируем определения компонентов с tf.keras.layers.MultiHeadAttention ---
def get_angles(pos, i, d_model): angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model)); return pos * angle_rates
def positional_encoding(position, d_model):
    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)
    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2]); angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
    pos_encoding = angle_rads[np.newaxis, ...]; return tf.cast(pos_encoding, dtype=tf.float32)
def create_padding_mask(seq): seq = tf.cast(tf.math.equal(seq, PAD_TOKEN_ID), tf.float32); return seq[:, tf.newaxis, tf.newaxis, :]
def create_look_ahead_mask(size): mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0); return mask
def create_masks(inp, tar):
    enc_padding_mask = create_padding_mask(inp); dec_padding_mask = create_padding_mask(inp)
    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1]); dec_target_padding_mask = create_padding_mask(tar)
    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask); return enc_padding_mask, combined_mask, dec_padding_mask
def point_wise_feed_forward_network(d_model, dff): return tf.keras.Sequential([tf.keras.layers.Dense(dff, activation='relu'), tf.keras.layers.Dense(d_model)], name="ffn_pw")

class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1, name="encoder_layer", **kwargs):
        super().__init__(name=name, **kwargs)
        self.d_model = d_model; self.num_heads = num_heads; self.dff = dff; self.rate = rate
        if d_model % num_heads != 0: raise ValueError(f"d_model({d_model}) % num_heads({num_heads}) != 0")
        key_dim = d_model // num_heads
        # Используем стандартный Keras MHA с dropout
        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, dropout=rate, name="mha")
        self.ffn = point_wise_feed_forward_network(d_model, dff)
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6); self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        # Убираем self.dropout1, т.к. dropout есть в MHA
        self.dropout_ffn = tf.keras.layers.Dropout(rate, name="dropout_ffn") # Оставляем dropout после FFN

    def call(self, x, training, mask):
        # Keras MHA ожидает attention_mask формы (..., seq_len_q, seq_len_k)
        attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask, training=training)
        # dropout1 не нужен
        out1 = self.layernorm1(x + attn_output)
        ffn_output = self.ffn(out1); ffn_output = self.dropout_ffn(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)
        return out2
    def get_config(self): config = super().get_config(); config.update({'d_model': self.d_model, 'num_heads': self.num_heads, 'dff': self.dff, 'rate': self.rate}); return config

class DecoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1, name="decoder_layer", **kwargs):
        super().__init__(name=name, **kwargs)
        self.d_model = d_model; self.num_heads = num_heads; self.dff = dff; self.rate = rate
        if d_model % num_heads != 0: raise ValueError(f"d_model({d_model}) % num_heads({num_heads}) != 0")
        key_dim = d_model // num_heads
        # Используем стандартный Keras MHA с dropout
        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, dropout=rate, name="masked_mha")
        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, dropout=rate, name="cross_mha")
        self.ffn = point_wise_feed_forward_network(d_model, dff)
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6); self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6); self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        # Убираем dropout1, dropout2
        self.dropout_ffn = tf.keras.layers.Dropout(rate, name="dropout_ffn") # Оставляем dropout после FFN

    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
        # Keras MHA ожидает маски соответствующей формы
        attn1 = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training)
        # dropout1 не нужен
        out1 = self.layernorm1(attn1 + x)
        attn2 = self.mha2(query=out1, value=enc_output, key=enc_output, attention_mask=padding_mask, training=training)
        # dropout2 не нужен
        out2 = self.layernorm2(attn2 + out1)
        ffn_output = self.ffn(out2); ffn_output = self.dropout_ffn(ffn_output, training=training)
        out3 = self.layernorm3(ffn_output + out2)
        return out3 # Не возвращаем веса внимания
    def get_config(self): config = super().get_config(); config.update({'d_model': self.d_model, 'num_heads': self.num_heads, 'dff': self.dff, 'rate': self.rate}); return config

class Encoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1, name="encoder", **kwargs):
        super().__init__(name=name, **kwargs); self.num_layers = num_layers; self.d_model = d_model; self.num_heads = num_heads; self.dff = dff
        self.input_vocab_size = input_vocab_size; self.maximum_position_encoding = maximum_position_encoding; self.rate = rate
        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model); self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)
        # Используем обновленный EncoderLayer
        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate, name=f'encoder_layer_{i}') for i in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(rate)
    def call(self, x, training, mask):
        seq_len = tf.shape(x)[1]; x = self.embedding(x); x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)); x += self.pos_encoding[:, :seq_len, :]
        x = self.dropout(x, training=training);
        for i in range(self.num_layers): x = self.enc_layers[i](x, training=training, mask=mask)
        return x
    def get_config(self): config = super().get_config(); config.update({'num_layers': self.num_layers, 'd_model': self.d_model, 'num_heads': self.num_heads, 'dff': self.dff, 'input_vocab_size': self.input_vocab_size, 'maximum_position_encoding': self.maximum_position_encoding, 'rate': self.rate}); return config

class Decoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1, name="decoder", **kwargs):
        super().__init__(name=name, **kwargs); self.num_layers = num_layers; self.d_model = d_model; self.num_heads = num_heads; self.dff = dff
        self.target_vocab_size = target_vocab_size; self.maximum_position_encoding = maximum_position_encoding; self.rate = rate
        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model); self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)
        # Используем обновленный DecoderLayer
        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate, name=f'decoder_layer_{i}') for i in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(rate)
    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
        seq_len = tf.shape(x)[1] # ; attention_weights = {} # Убрали веса
        x = self.embedding(x); x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)); x += self.pos_encoding[:, :seq_len, :]
        x = self.dropout(x, training=training)
        for i in range(self.num_layers):
            # DecoderLayer теперь возвращает только x
            x = self.dec_layers[i](x, enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)
            # attention_weights[f'decoder_layer{i+1}_block1'] = block1; attention_weights[f'decoder_layer{i+1}_block2'] = block2 # Убрали
        return x # Возвращаем только выход
    def get_config(self): config = super().get_config(); config.update({'num_layers': self.num_layers, 'd_model': self.d_model, 'num_heads': self.num_heads, 'dff': self.dff, 'target_vocab_size': self.target_vocab_size, 'maximum_position_encoding': self.maximum_position_encoding, 'rate': self.rate}); return config

class Transformer(tf.keras.Model):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1, name="transformer", **kwargs):
        super().__init__(name=name, **kwargs); self.num_layers = num_layers; self.d_model = d_model; self.num_heads = num_heads; self.dff = dff
        self.input_vocab_size = input_vocab_size; self.target_vocab_size = target_vocab_size; self.pe_input = pe_input; self.pe_target = pe_target; self.rate = rate
        # Используем обновленные Encoder и Decoder
        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate, name='encoder')
        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate, name='decoder')
        self.final_layer = tf.keras.layers.Dense(target_vocab_size, name='final_dense_layer')
    def call(self, inputs, training=None):
        if not isinstance(inputs, (list, tuple)) or len(inputs)!= 2: raise ValueError("Ожидается кортеж/список (inp, tar)")
        inp, tar = inputs
        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar)
        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)
        # Decoder теперь возвращает только выход
        dec_output = self.decoder(tar, enc_output=enc_output, training=training, look_ahead_mask=combined_mask, padding_mask=dec_padding_mask)
        final_output = self.final_layer(dec_output)
        return final_output # Возвращаем только логиты
    def get_config(self): config = {'num_layers': self.num_layers, 'd_model': self.d_model, 'num_heads': self.num_heads, 'dff': self.dff, 'input_vocab_size': self.input_vocab_size, 'target_vocab_size': self.target_vocab_size, 'pe_input': self.pe_input, 'pe_target': self.pe_target, 'rate': self.rate}; return config
    @classmethod
    def from_config(cls, config): return cls(**config)

print("Архитектура модели (со стандартным MHA) определена.")
print("-" * 30 + "\n")

# ==============================================================================
# === 5. Оптимизатор, Потери, Метрики (ДЛЯ КАСТОМНОГО ЦИКЛА) ==================
# ==============================================================================
print("--- Определение Оптимизатора, Потерь, Метрик для кастомного цикла ---")

# --- CustomSchedule ---
class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, d_model, warmup_steps=4000):
        super(CustomSchedule, self).__init__(); self.d_model = tf.cast(d_model, tf.float32); self.warmup_steps = tf.cast(warmup_steps, tf.float32)
    def __call__(self, step):
        step = tf.cast(step, tf.float32); arg1 = tf.math.rsqrt(tf.maximum(step, 1.0)); arg2 = step * (self.warmup_steps ** -1.5)
        lr = tf.math.rsqrt(self.d_model + 1e-9) * tf.math.minimum(arg1, arg2); return tf.math.maximum(lr, 1e-7)
    def get_config(self): return {"d_model": float(self.d_model), "warmup_steps": float(self.warmup_steps)}

# --- Функции потерь и точности ---
def masked_loss(real, pred, loss_object_local):
    mask = tf.math.logical_not(tf.math.equal(real, PAD_TOKEN_ID)); loss_ = loss_object_local(real, pred)
    mask = tf.cast(mask, dtype=loss_.dtype); loss_ *= mask
    return tf.reduce_sum(loss_) / (tf.reduce_sum(mask) + 1e-9)

def masked_accuracy(real, pred):
    accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), dtype=real.dtype))
    mask = tf.math.logical_not(tf.math.equal(real, PAD_TOKEN_ID)); accuracies = tf.math.logical_and(mask, accuracies)
    accuracies = tf.cast(accuracies, dtype=tf.float32); mask = tf.cast(mask, dtype=tf.float32)
    sum_mask = tf.reduce_sum(mask)
    return tf.where(sum_mask > 0, tf.reduce_sum(accuracies) / sum_mask, tf.constant(0.0, dtype=tf.float32))

print("Определены.")
print("-" * 30 + "\n")

# ==============================================================================
# === 6. Обучение (УПРОЩЕННЫЙ КАСТОМНЫЙ ЦИКЛ) ===============================
# ==============================================================================
print("--- Настройка Обучения (упрощенный кастомный цикл) ---")
transformer_main = None; optimizer = None; loss_object = None
ckpt = None; ckpt_manager = None; initial_epoch = 0

with strategy.scope():
    print("Создание модели, оптимизатора, потерь и чекпоинта в strategy.scope()...")
    transformer_main = Transformer( # Архитектура ~70M со стандартным MHA
        num_layers=NUM_LAYERS, d_model=D_MODEL, num_heads=NUM_HEADS, dff=DFF,
        input_vocab_size=VOCAB_SIZE, target_vocab_size=VOCAB_SIZE,
        pe_input=MAX_LENGTH, pe_target=MAX_LENGTH, rate=DROPOUT_RATE )
    print("Модель создана.")
    learning_rate_schedule = CustomSchedule(D_MODEL, warmup_steps=WARMUP_STEPS)
    optimizer = tf.keras.optimizers.Adam(learning_rate_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)
    print("Оптимизатор создан.")
    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
    print("Объект потерь создан.")
    print("Построение модели..."); dummy_enc_inp = tf.zeros((1, MAX_LENGTH), dtype=tf.int32); dummy_dec_inp = tf.zeros((1, MAX_LENGTH - 1), dtype=tf.int32)
    try: _ = transformer_main((dummy_enc_inp, dummy_dec_inp), training=False); print("Модель построена.")
    except Exception as e_build: print(f"!!! ОШИБКА построения: {e_build}"); traceback.print_exc(); raise RuntimeError(f"Ошибка построения: {e_build}")
    print(f"Настройка чекпоинтов TF в {CHECKPOINT_DIR}..."); ckpt = tf.train.Checkpoint(transformer=transformer_main, optimizer=optimizer)
    ckpt_manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_DIR, max_to_keep=5)
    if ckpt_manager.latest_checkpoint and EPOCHS > 0:
        try:
             status = ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()
             print(f"Восстановлен чекпоинт: {ckpt_manager.latest_checkpoint}"); loaded_step = optimizer.iterations.numpy()
             try: steps_per_epoch_est = len(train_dataset)
             except TypeError: steps_per_epoch_est = NUM_EXAMPLES // GLOBAL_BATCH_SIZE
             initial_epoch = loaded_step // steps_per_epoch_est if steps_per_epoch_est > 0 else 0
             print(f"Восстановлен шаг: {loaded_step}. Начальная эпоха: {initial_epoch + 1}")
        except Exception as e_restore: print(f"Ошибка восстановления: {e_restore}. Начинаем с нуля."); initial_epoch = 0
    else: print("Чекпоинт не найден или обучение пропускается."); initial_epoch = 0

print("Создание/загрузка объектов внутри scope завершено.")

print("\n--- Сводка по модели ---")
transformer_main.summary(line_length=120)
print(f"Количество параметров: ~{transformer_main.count_params() // 1_000_000}M")
print("-------------------------\n")

# === ЗАМЕНИ СТАРЫЕ ФУНКЦИИ НА ЭТИ ===

@tf.function
def train_step(inputs):
    (enc_in, dec_in), dec_out = inputs
    with tf.GradientTape() as tape:
        predictions = transformer_main((enc_in, dec_in), training=True)
        loss = masked_loss(dec_out, predictions, loss_object)
    gradients = tape.gradient(loss, transformer_main.trainable_variables)

    # Вычисляем булев флаг NaN/Inf
    has_nan_inf_bool = tf.reduce_any([tf.reduce_any(tf.math.is_inf(g)) | tf.reduce_any(tf.math.is_nan(g)) for g in gradients if g is not None])

    # ===> ИЗМЕНЕНИЕ 1: Преобразуем в int32 ЗДЕСЬ (внутри реплики) <===
    has_nan_inf_int = tf.cast(has_nan_inf_bool, tf.int32)

    # Применяем градиенты, только если нет NaN/Inf (используем исходный булев флаг)
    if not has_nan_inf_bool:
        trainable_vars = transformer_main.trainable_variables
        grads_and_vars = [(g, v) for g, v in zip(gradients, trainable_vars) if g is not None]
        if grads_and_vars: optimizer.apply_gradients(grads_and_vars)

    # ===> ИЗМЕНЕНИЕ 2: Возвращаем целочисленный флаг <===
    return loss, has_nan_inf_int, dec_out, predictions

# ----------------------------------------------------------

@tf.function
def distributed_train_step_caller(dist_inputs):
    # Получаем результаты от каждой реплики
    # ===> ИЗМЕНЕНИЕ 3: Ожидаем целочисленные флаги (_int) <===
    per_replica_losses, per_replica_nan_flags_int, per_replica_dec_out, per_replica_preds = strategy.run(train_step, args=(dist_inputs,))

    # Усредняем потери по репликам
    mean_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses, axis=None)

    # ===> ИЗМЕНЕНИЕ 4: Суммируем целочисленные флаги напрямую (без tf.cast) <===
    any_nan_int = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_nan_flags_int, axis=None)
    # Преобразуем сумму в итоговый булев флаг
    any_nan = any_nan_int > 0

    # Обновляем метрики только если НЕ было NaN ни на одной реплике
    if not any_nan:
        gathered_dec_out = strategy.gather(per_replica_dec_out, axis=0)
        gathered_preds = strategy.gather(per_replica_preds, axis=0)
        accuracy = masked_accuracy(gathered_dec_out, gathered_preds)
        train_loss_metric.update_state(mean_loss)
        train_accuracy_metric.update_state(accuracy)

    # Возвращаем среднюю потерю и итоговый булев флаг NaN
    return mean_loss, any_nan

# === КОНЕЦ БЛОКА ДЛЯ ЗАМЕНЫ ===

if EPOCHS > 0:
    print("--- Начало Обучения (упрощенный кастомный цикл, стандартный MHA) ---")
    start_time_total = time.time()
    try: steps_per_epoch = len(train_dataset)
    except TypeError: steps_per_epoch = NUM_EXAMPLES // GLOBAL_BATCH_SIZE; print(f"Оценка шагов: {steps_per_epoch}")
    if steps_per_epoch <= 0: raise RuntimeError("Шагов на эпоху <= 0!")
    print(f"Шагов в эпохе: {steps_per_epoch}")

    train_loss_metric = tf.keras.metrics.Mean(name='train_loss')
    train_accuracy_metric = tf.keras.metrics.Mean(name='train_accuracy')
    nan_detected_in_training = False

    for epoch in range(initial_epoch, EPOCHS):
        start_time_epoch = time.time()
        train_loss_metric.reset_state()
        train_accuracy_metric.reset_state()
        print(f"\n[INFO] Эпоха {epoch + 1}/{EPOCHS}.")

        for batch_num, batch_data in enumerate(train_dataset_dist):
            # Вызываем обновленный distributed_train_step_caller
            loss_value, has_nan = distributed_train_step_caller(batch_data)

            if has_nan: print(f"\n!!! NaN обнаружен на эпохе {epoch+1}, батче {batch_num+1}. Прерывание обучения. !!!"); nan_detected_in_training = True; break
            log_freq = 100; current_iter = optimizer.iterations.numpy()
            if batch_num == 0 or current_iter % log_freq == 0 or batch_num == steps_per_epoch - 1:
                current_lr = learning_rate_schedule(tf.cast(current_iter, tf.float32)).numpy()
                print(f'  Эпоха {epoch + 1} Батч {batch_num + 1}/{steps_per_epoch} (Шаг {current_iter}) '
                      f'Loss: {train_loss_metric.result():.4f} Accuracy: {train_accuracy_metric.result():.4f} LR: {current_lr:.7f}')

        epoch_time = time.time() - start_time_epoch
        if nan_detected_in_training: print(f'\nЭпоха {epoch + 1} ПРЕРВАНА из-за NaN.'); break
        print(f'\nЭпоха {epoch + 1} Завершена.'); print(f'  Loss: {train_loss_metric.result():.4f}'); print(f'  Accuracy: {train_accuracy_metric.result():.4f}'); print(f'  Время: {epoch_time:.2f} сек')
        if (epoch + 1) % CHECKPOINT_SAVE_FREQ == 0 or (epoch + 1) == EPOCHS:
            ckpt_save_path = ckpt_manager.save(); print(f'Сохранен чекпоинт TF {ckpt_save_path} (Шаг {optimizer.iterations.numpy()})')

    total_time = time.time() - start_time_total
    print(f"\n--- Обучение Завершено ---"); print(f"Общее время: {total_time:.2f} сек ({total_time/60:.2f} мин)")
    if transformer_main is not None and not nan_detected_in_training:
        print(f"\nСохранение финальной модели Keras в {FINAL_MODEL_PATH}...")
        try: transformer_main.save(FINAL_MODEL_PATH, save_format='keras'); print("Модель Keras сохранена.")
        except Exception as e_save_final: print(f"!!! ОШИБКА сохранения: {e_save_final}"); traceback.print_exc()
    elif nan_detected_in_training: print("Модель не сохранена из-за NaN.")
    else: print("Ошибка: Модель для сохранения не найдена.")

elif EPOCHS == 0:
    print("\nОбучение пропущено (EPOCHS = 0).")
    if transformer_main is None and ckpt_manager.latest_checkpoint:
         print("Загрузка из чекпоинта TF...")
         try:
             with strategy.scope():
                 transformer_main = Transformer(...) # Создаем модель
                 temp_optimizer = tf.keras.optimizers.Adam(0.0)
                 temp_ckpt = tf.train.Checkpoint(transformer=transformer_main, optimizer=temp_optimizer)
                 status = temp_ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()
             print(f"Модель восстановлена из {ckpt_manager.latest_checkpoint}.")
             print("Построение..."); dummy_enc=tf.zeros((1,MAX_LENGTH),dtype=tf.int32); dummy_dec=tf.zeros((1,MAX_LENGTH-1),dtype=tf.int32); _ = transformer_main((dummy_enc, dummy_dec), training=False); print("Построена.")
         except Exception as e: print(f"Ошибка восстановления: {e}"); transformer_main = None
    elif transformer_main is None: print("Модель не найдена.")

# ==============================================================================
# === 7. Интерактивный Чат (Адаптированный, БЕЗ штрафа) ========================
# ==============================================================================
print("\n--- Запуск Интерактивного Чата ---")
if transformer_main is None: print("Невозможно запустить чат: модель недоступна.")
elif token_ids is None: print("Невозможно запустить чат: ошибка токенов.")
else:
    print("\nЗапуск чата...");
    try:
        # --- Функции чата ---
        def decode_sequence(sequence, tokenizer_inf):
            words = []; start_id = tokenizer_inf.word_index.get('<start>')
            start_idx = 1 if sequence and sequence[0] == start_id else 0
            for idx in sequence[start_idx:]:
                idx_int = int(idx);
                if idx_int == PAD_TOKEN_ID: continue
                word = tokenizer_inf.index_word.get(idx_int, '<unk>')
                if word == '<end>': break; words.append(word)
            return " ".join(words).replace('<unk>', '...')
        @tf.function
        def evaluate_tf(encoder_input_tensor, start_token_id_inf, end_token_id_inf, max_length, temperature):
            start_token_id_inf=tf.cast(start_token_id_inf,dtype=tf.int32);end_token_id_inf=tf.cast(end_token_id_inf,dtype=tf.int32)
            max_length=tf.cast(max_length,dtype=tf.int32);temperature=tf.cast(temperature,dtype=tf.float32)
            i=tf.constant(0,dtype=tf.int32);decoder_input_ids=tf.expand_dims([start_token_id_inf],0)
            generated_ids_array=tf.TensorArray(dtype=tf.int32,size=0,dynamic_size=True);finished=tf.constant(False,dtype=tf.bool)
            def cond(i,d_in_ids,gen_arr,fin): return tf.logical_and(i<max_length,tf.logical_not(fin))
            def body(i,d_in_ids,gen_arr,fin):
                predictions=transformer_main((encoder_input_tensor,d_in_ids),training=False);last_token_logits=predictions[:,-1,:]
                def sample_w_temp(): scaled=last_token_logits/tf.maximum(temperature,1e-9);return tf.random.categorical(scaled,num_samples=1,dtype=tf.int32)[0,0]
                def sample_greedy(): return tf.argmax(last_token_logits,axis=-1,output_type=tf.int32)[0]
                pred_id=tf.cond(tf.equal(temperature,0.0),true_fn=sample_greedy,false_fn=sample_w_temp)
                curr_fin=tf.equal(pred_id,end_token_id_inf);write_idx=i;gen_arr_upd=tf.cond(tf.logical_not(curr_fin),lambda:gen_arr.write(write_idx,pred_id),lambda:gen_arr)
                new_d_in_ids=tf.concat([d_in_ids,[[pred_id]]],axis=-1);new_i=i+1;new_fin=tf.logical_or(fin,curr_fin);return new_i,new_d_in_ids,gen_arr_upd,new_fin
            final_state=tf.while_loop(cond,body,loop_vars=[i,decoder_input_ids,generated_ids_array,finished],shape_invariants=[i.get_shape(),tf.TensorShape([1,None]),tf.TensorShape(None),finished.get_shape()])
            result=tf.concat([[start_token_id_inf],final_state[2].stack()],axis=0);return result
        def evaluate(sentence, transformer_model_inf, tokenizer_inf, start_token_id_inf, end_token_id_inf, max_length=MAX_LENGTH, temperature=1.0):
            if not start_token_id_inf or not end_token_id_inf: return "[Ошибка: токены start/end не найдены]"
            sentence_proc=preprocess_sentence(sentence);inputs=tokenizer_inf.texts_to_sequences([sentence_proc])
            if not inputs or not inputs[0]: return "[Не понял ваш ввод. Попробуйте перефразировать.]"
            inputs_pad=tf.keras.preprocessing.sequence.pad_sequences(inputs,maxlen=max_length,padding='post',truncating='post');encoder_in_tensor=tf.convert_to_tensor(inputs_pad,dtype=tf.int32)
            if tf.rank(encoder_in_tensor)!=2 or tf.shape(encoder_in_tensor)[0]!=1:
                if tf.rank(encoder_in_tensor)==1: encoder_in_tensor=tf.expand_dims(encoder_in_tensor,0)
                if tf.rank(encoder_in_tensor)!=2 or tf.shape(encoder_in_tensor)[0]!=1: return "[Ошибка: Некорректная форма входа]"
                else: return "[Ошибка: Некорректная форма входа]"
            pred_ids=evaluate_tf(encoder_in_tensor,start_token_id_inf,end_token_id_inf,max_length,temperature)
            pred_seq=pred_ids.numpy();pred_sent=decode_sequence(pred_seq,tokenizer_inf);return pred_sent
        def chat(transformer_inf, tokenizer_inf, start_token_id_inf, end_token_id_inf, temperature=DEFAULT_TEMPERATURE):
            print("\n--- Начинаем чат! ---");print(f"Температура: {temperature}. Введите 'quit' или 'exit', чтобы завершить.")
            while True:
                try:user_input=input("Вы: ")
                except EOFError: print("\nВыход.");break
                if user_input.lower() in ['quit','exit']: break
                if not user_input.strip(): continue
                start_t=time.time()
                try: response=evaluate(user_input,transformer_inf,tokenizer_inf,start_token_id_inf,end_token_id_inf,max_length=MAX_LENGTH,temperature=temperature)
                except Exception as e_eval: print(f"\n!!! Ошибка во время evaluate: {e_eval}"); traceback.print_exc(); response = "[Произошла ошибка генерации ответа]"
                eval_t=time.time()-start_t; print(f"Модель (t={temperature:.1f}, {eval_t:.2f}s): {response}"); print()

        # Запуск чата
        chat(transformer_main, tokenizer, START_TOKEN_ID, END_TOKEN_ID, temperature=DEFAULT_TEMPERATURE)

    except Exception as e_chat: print(f"\nОшибка во время чата: {e_chat}"); traceback.print_exc()

print("\nПрограмма завершена.")