# -*- coding: utf-8 -*-

# ==============================================================================
# === 0. Импорты и Установка (TPU) ============================================
# ==============================================================================
print("Импорт библиотек и Установка зависимостей для TPU...")
# Установка (выполнить один раз в среде):
!pip install datasets numpy pickle5 tensorflow==2.18.0 tensorflow-tpu==2.18.0 --find-links=https://storage.googleapis.com/libtpu-tf-releases/index.html

import tensorflow as tf
import numpy as np
import re
import time
import os
import pickle
from datasets import load_dataset
import shutil
import traceback

print(f"TensorFlow version: {tf.__version__}") # Должен быть 2.18.0 для TPU
try:
    import datasets
    print(f"Datasets version: {datasets.__version__}")
except ImportError:
    print("Библиотека datasets не установлена. Пожалуйста, установите: pip install datasets")
    raise RuntimeError("Библиотека datasets не установлена.")

# ==============================================================================
# === 0.1 Инициализация TPU и Стратегии =======================================
# ==============================================================================
print("\nИнициализация TPU и стратегии...")
strategy = None
try:
    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')
    print(f"Подключение к TPU: {resolver.master()}")
    tf.config.experimental_connect_to_cluster(resolver)
    tf.tpu.experimental.initialize_tpu_system(resolver)
    strategy = tf.distribute.TPUStrategy(resolver)
    print("TPU инициализирована успешно.")
    print(f"Количество реплик (TPU ядер): {strategy.num_replicas_in_sync}")

    # --- Установка Mixed Precision ---
    # ОСТАВЛЕНО ЗАКОММЕНТИРОВАННЫМ (используется float32).
    # policy_name = 'mixed_bfloat16'
    # policy = tf.keras.mixed_precision.Policy(policy_name)
    # tf.keras.mixed_precision.set_global_policy(policy)
    # print(f"Установлена политика Mixed Precision: {tf.keras.mixed_precision.global_policy().name}")

except ValueError as e:
    print(f"Ошибка инициализации TPU: {e}. TPU недоступна.")
    print("Убедитесь, что среда выполнения настроена на использование TPU.")
    raise RuntimeError(f"Ошибка инициализации TPU: {e}. TPU недоступна.")

if strategy is None:
     raise RuntimeError("Не удалось определить стратегию TPU.")

print(f"\nИтоговая стратегия: {strategy.__class__.__name__}")
print(f"Итоговое количество реплик: {strategy.num_replicas_in_sync}")
print(f"Итоговая политика точности: {tf.keras.mixed_precision.global_policy().name}\n") # Будет float32

# ==============================================================================
# === 1. Параметры =============================================================
# ==============================================================================
print("Установка параметров...")

# --- Параметры данных ---
DATASET_NAME = "daily_dialog"
DATA_DIR = "chatbot_data_tpu_complex"
TOKENIZER_FILENAME = "tokenizer_tpu_complex.pickle"
NUM_EXAMPLES = 120000
MAX_LENGTH = 40
BUFFER_SIZE = NUM_EXAMPLES

# --- Подбор BATCH_SIZE для TPU ---
PER_REPLICA_BATCH_SIZE = 128
GLOBAL_BATCH_SIZE = PER_REPLICA_BATCH_SIZE * strategy.num_replicas_in_sync
print(f"Per-Replica Batch Size: {PER_REPLICA_BATCH_SIZE}")
print(f"Global Batch Size: {GLOBAL_BATCH_SIZE}")

VOCAB_SIZE_LIMIT = 20000

# --- Параметры модели (УСЛОЖНЕНИЕ) ---
NUM_LAYERS = 6
D_MODEL = 512
NUM_HEADS = 8
DFF = 2048
DROPOUT_RATE = 0.1

# --- Параметры обучения ---
EPOCHS = 50
WARMUP_STEPS = 16000
CLIPNORM = 0.2
PEAK_LR_SCALE = 0.5 # Множитель для уменьшения пикового LR

# --- Параметры чата ---
DEFAULT_TEMPERATURE = 0.7


# ==============================================================================
# === 2. Функции Предобработки Данных ==========================================
# (Код без изменений)
# ==============================================================================
def preprocess_sentence(w):
    if not isinstance(w, str): w = str(w)
    w = w.lower().strip()
    w = re.sub(r"([?.!,])", r" \1 ", w); w = re.sub(r'[" "]+', " ", w); w = w.strip()
    w = '<start> ' + w + ' <end>'; return w
def create_pairs_from_hf_dataset(dataset, num_examples, max_length):
    inputs, outputs = [], []; processed_dialogs = 0
    print("Извлечение пар вопрос-ответ из датасета..."); total_dialogs = len(dataset)
    skipped_count = 0; stop_processing = False
    for example in dataset:
        if stop_processing: break
        processed_dialogs += 1; dialog = example.get('dialog', [])
        if isinstance(dialog, list) and len(dialog) > 1:
            for i in range(len(dialog) - 1):
                inp_raw = dialog[i]; out_raw = dialog[i+1]
                if not inp_raw or not out_raw or not isinstance(inp_raw, str) or not isinstance(out_raw, str): skipped_count += 1; continue
                inp = preprocess_sentence(inp_raw); out = preprocess_sentence(out_raw)
                inp_len = len(inp.split()); out_len = len(out.split())
                if inp_len > 2 and inp_len <= max_length and out_len > 2 and out_len <= max_length:
                    inputs.append(inp); outputs.append(out)
                    if len(inputs) >= num_examples:
                        print(f"\nДостигнут лимит в {num_examples} пар.");
                        if skipped_count > 0: print(f"(Пропущено неподходящих пар: {skipped_count})")
                        stop_processing = True; break
                else: skipped_count += 1
        if stop_processing: break
        if processed_dialogs % 1000 == 0 or processed_dialogs == total_dialogs:
             print(f"Обработано диалогов: {processed_dialogs}/{total_dialogs}, Собрано пар: {len(inputs)}/{num_examples}, Пропущено: {skipped_count}", end='\r')
    print(f"\nОбработка диалогов завершена. Сгенерировано {len(inputs)} пар.");
    if skipped_count > 0: print(f"(Всего пропущено неподходящих пар: {skipped_count})")
    return inputs, outputs
def load_and_prepare_data(dataset_name, num_examples, max_length, buffer_size, vocab_limit, data_dir, tokenizer_filename, global_batch_size):
    print("\n--- Загрузка и обработка данных ---")
    if not os.path.exists(data_dir): print(f"Создание директории: {data_dir}"); os.makedirs(data_dir)
    try:
        print(f"Загрузка '{dataset_name}'..."); raw_datasets = load_dataset(dataset_name, trust_remote_code=True)
        all_dialogs = [];
        for split in raw_datasets.keys(): print(f"Используем диалоги из '{split}' ({len(raw_datasets[split])})"); all_dialogs.extend(raw_datasets[split])
        print(f"Всего диалогов для обработки: {len(all_dialogs)}")
    except Exception as e: print(f"Ошибка при загрузке датасета '{dataset_name}': {e}"); traceback.print_exc(); return None, None, None, None
    questions, answers = create_pairs_from_hf_dataset(all_dialogs, num_examples, max_length)
    if not questions: print("Не удалось создать пары вопрос-ответ."); return None, None, None, None
    print(f"Создано {len(questions)} пар.");
    if questions: print("Пример пары:"); print(f"  Вопрос: {questions[0]}"); print(f"  Ответ: {answers[0]}")
    tokenizer_path = os.path.join(data_dir, tokenizer_filename); tokenizer = None
    if os.path.exists(tokenizer_path):
        print(f"Загрузка токенизатора из {tokenizer_path}...")
        try:
            with open(tokenizer_path, 'rb') as handle: tokenizer = pickle.load(handle); print("Токенизатор загружен.")
        except Exception as e: print(f"Ошибка загрузки токенизатора: {e}. Создаем новый."); tokenizer = None
    else: print("Токенизатор не найден. Создаем новый."); tokenizer = None
    if tokenizer is None:
        print("Создание и сохранение нового токенизатора...")
        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_limit if vocab_limit else None, oov_token='<unk>', filters='')
        tokenizer.fit_on_texts(questions + answers)
        try:
            with open(tokenizer_path, 'wb') as handle: pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL); print(f"Токенизатор сохранен в {tokenizer_path}")
        except Exception as e: print(f"Ошибка сохранения токенизатора: {e}")
    actual_vocab_size = len(tokenizer.word_index) + 1
    vocab_size = min(vocab_limit + 1, actual_vocab_size) if vocab_limit else actual_vocab_size
    print(f"Размер словаря (с OOV и паддингом 0): {vocab_size}")
    start_token_id = tokenizer.word_index.get('<start>'); end_token_id = tokenizer.word_index.get('<end>')
    if start_token_id is None or end_token_id is None: print("Критическая ошибка: не найдены <start> или <end> токены в словаре!"); return None, tokenizer, vocab_size, None
    print(f"ID <start>: {start_token_id}, ID <end>: {end_token_id}")
    print("Преобразование в последовательности и паддинг...")
    try: question_seqs = tokenizer.texts_to_sequences(questions); answer_seqs = tokenizer.texts_to_sequences(answers)
    except Exception as e: print(f"Ошибка токенизации текстов: {e}"); return None, tokenizer, vocab_size, (start_token_id, end_token_id)
    question_seqs = tf.keras.preprocessing.sequence.pad_sequences(question_seqs, maxlen=max_length, padding='post', truncating='post')
    answer_seqs = tf.keras.preprocessing.sequence.pad_sequences(answer_seqs, maxlen=max_length, padding='post', truncating='post')
    print(f"Форма вопросов после паддинга: {question_seqs.shape}"); print(f"Форма ответов после паддинга: {answer_seqs.shape}")
    if question_seqs.shape[0] == 0: print("Ошибка: Нет данных для создания Dataset после обработки."); return None, tokenizer, vocab_size, (start_token_id, end_token_id)
    encoder_inputs = tf.cast(question_seqs, tf.int32); decoder_inputs = tf.cast(answer_seqs[:, :-1], tf.int32); decoder_outputs = tf.cast(answer_seqs[:, 1:], tf.int32)
    print(f"Форма входа энкодера: {encoder_inputs.shape}"); print(f"Форма входа декодера: {decoder_inputs.shape}"); print(f"Форма выхода декодера: {decoder_outputs.shape}")
    dataset = tf.data.Dataset.from_tensor_slices(((encoder_inputs, decoder_inputs), decoder_outputs))
    dataset = dataset.cache(); dataset = dataset.shuffle(buffer_size); dataset = dataset.batch(global_batch_size); dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)
    print("tf.data.Dataset создан и оптимизирован.")
    print("\nПример обработанных данных (первый элемент первого батча):")
    try:
        for (enc_in_batch, dec_in_batch), dec_out_batch in dataset.take(1):
            print(f"  Структура батча: ((enc_in, dec_in), dec_out)"); print(f"  Encoder Input IDs shape: {enc_in_batch.shape}"); print(f"  Decoder Input IDs shape: {dec_in_batch.shape}"); print(f"  Decoder Target IDs shape: {dec_out_batch.shape}")
            enc_in_sample = enc_in_batch[0].numpy(); dec_in_sample = dec_in_batch[0].numpy(); dec_out_sample = dec_out_batch[0].numpy()
            def _decode(seq):
                 if hasattr(tokenizer, 'index_word'): return " ".join([tokenizer.index_word.get(i, '?') for i in seq if i != 0])
                 else: return "[Ошибка: tokenizer.index_word не найден]"
            print(f"  Encoder Input Text (sample): {_decode(enc_in_sample)}"); print(f"  Decoder Input Text (sample): {_decode(dec_in_sample)}"); print(f"  Decoder Target Text (sample): {_decode(dec_out_sample)}")
            break
    except Exception as e: print(f"Не удалось получить пример батча: {e}")
    return dataset, tokenizer, vocab_size, (start_token_id, end_token_id)


# ==============================================================================
# === 3. Компоненты Модели Трансформер ========================================
# ==============================================================================
print("\nОпределение компонентов модели Трансформер...")
def get_angles(pos, i, d_model): angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model)); return pos * angle_rates
def positional_encoding(position, d_model): angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model); angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2]); angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2]); pos_encoding = angle_rads[np.newaxis, ...]; return tf.cast(pos_encoding, dtype=tf.float32)
def create_padding_mask(seq): seq = tf.cast(tf.math.equal(seq, 0), tf.float32); return seq[:, tf.newaxis, tf.newaxis, :]
def create_look_ahead_mask(size): mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0); return mask
def create_masks(inp, tar): enc_padding_mask = create_padding_mask(inp); dec_padding_mask = create_padding_mask(inp); look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1]); dec_target_padding_mask = create_padding_mask(tar); combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask); return enc_padding_mask, combined_mask, dec_padding_mask
def scaled_dot_product_attention(q, k, v, mask):
    compute_dtype = q.dtype; matmul_qk = tf.matmul(q, k, transpose_b=True)
    dk = tf.cast(tf.shape(k)[-1], compute_dtype); scaled_attention_logits = matmul_qk / tf.math.sqrt(dk + tf.cast(1e-9, compute_dtype))
    if mask is not None: mask = tf.cast(mask, compute_dtype); scaled_attention_logits += (mask * tf.cast(-1e9, compute_dtype))
    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1); output = tf.matmul(attention_weights, v); return output, attention_weights
class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, **kwargs):
        super().__init__(**kwargs); self.num_heads = num_heads; self.d_model = d_model
        if d_model % self.num_heads != 0: raise ValueError(f"d_model ({d_model}) must be divisible by num_heads ({num_heads})")
        self.depth = d_model // self.num_heads; self.wq = tf.keras.layers.Dense(d_model); self.wk = tf.keras.layers.Dense(d_model); self.wv = tf.keras.layers.Dense(d_model); self.dense = tf.keras.layers.Dense(d_model)
    def split_heads(self, x, batch_size): x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)); return tf.transpose(x, perm=[0, 2, 1, 3])
    def call(self, v, k, q, mask=None):
        batch_size = tf.shape(q)[0]; q = self.wq(q); k = self.wk(k); v = self.wv(v)
        q = self.split_heads(q, batch_size); k = self.split_heads(k, batch_size); v = self.split_heads(v, batch_size)
        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)
        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]); concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))
        output = self.dense(concat_attention); return output, attention_weights
    def get_config(self): config = super().get_config(); config.update({'d_model': self.d_model, 'num_heads': self.num_heads}); return config
def point_wise_feed_forward_network(d_model, dff): return tf.keras.Sequential([tf.keras.layers.Dense(dff, activation='relu'), tf.keras.layers.Dense(d_model)], name='ffn')

# <<< ИЗМЕНЕНИЕ: Сигнатура call изменена >>>
class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):
        super().__init__(**kwargs); self.d_model = d_model; self.num_heads = num_heads; self.dff = dff; self.rate = rate
        self.mha = MultiHeadAttention(d_model, num_heads, name='enc_mha'); self.ffn = point_wise_feed_forward_network(d_model, dff)
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='enc_ln_1'); self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='enc_ln_2')
        self.dropout1 = tf.keras.layers.Dropout(rate); self.dropout2 = tf.keras.layers.Dropout(rate)
    # <<< ИЗМЕНЕНИЕ: training и mask передаются как keyword arguments >>>
    def call(self, x, mask, training=None):
        attn_output, _ = self.mha(v=x, k=x, q=x, mask=mask)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)
        return out2
    def get_config(self): config = super().get_config(); config.update({'d_model': self.d_model, 'num_heads': self.num_heads, 'dff': self.dff, 'rate': self.rate}); return config

# <<< ИЗМЕНЕНИЕ: Сигнатура call изменена >>>
class DecoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):
        super().__init__(**kwargs); self.d_model = d_model; self.num_heads = num_heads; self.dff = dff; self.rate = rate
        self.mha1 = MultiHeadAttention(d_model, num_heads, name='dec_masked_mha'); self.mha2 = MultiHeadAttention(d_model, num_heads, name='dec_cross_mha')
        self.ffn = point_wise_feed_forward_network(d_model, dff); self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='dec_ln_1')
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='dec_ln_2'); self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='dec_ln_3')
        self.dropout1 = tf.keras.layers.Dropout(rate); self.dropout2 = tf.keras.layers.Dropout(rate); self.dropout3 = tf.keras.layers.Dropout(rate)
    # <<< ИЗМЕНЕНИЕ: training и маски передаются как keyword arguments >>>
    def call(self, x, enc_output, look_ahead_mask, padding_mask, training=None):
        attn1, attn_weights_block1 = self.mha1(v=x, k=x, q=x, mask=look_ahead_mask)
        attn1 = self.dropout1(attn1, training=training)
        out1 = self.layernorm1(attn1 + x)
        attn2, attn_weights_block2 = self.mha2(v=enc_output, k=enc_output, q=out1, mask=padding_mask)
        attn2 = self.dropout2(attn2, training=training)
        out2 = self.layernorm2(attn2 + out1)
        ffn_output = self.ffn(out2)
        ffn_output = self.dropout3(ffn_output, training=training)
        out3 = self.layernorm3(ffn_output + out2)
        return out3, attn_weights_block1, attn_weights_block2
    def get_config(self): config = super().get_config(); config.update({'d_model': self.d_model, 'num_heads': self.num_heads, 'dff': self.dff, 'rate': self.rate}); return config

# <<< ИЗМЕНЕНИЕ: Сигнатура call изменена >>>
class Encoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1, **kwargs):
        super().__init__(name='encoder', **kwargs); self.num_layers = num_layers; self.d_model = d_model
        self.num_heads = num_heads; self.dff = dff; self.input_vocab_size = input_vocab_size
        self.maximum_position_encoding = maximum_position_encoding; self.rate = rate
        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model); self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)
        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate, name=f'encoder_layer_{i}') for i in range(num_layers)]; self.dropout = tf.keras.layers.Dropout(rate)
    # <<< ИЗМЕНЕНИЕ: training и mask передаются как keyword arguments >>>
    def call(self, x, mask, training=None):
        seq_len = tf.shape(x)[1]; x = self.embedding(x); embedding_scale = tf.math.sqrt(tf.cast(self.d_model, self.compute_dtype));
        x *= embedding_scale; pos_encoding_casted = tf.cast(self.pos_encoding[:, :seq_len, :], self.compute_dtype); x += pos_encoding_casted
        x = self.dropout(x, training=training)
        for i in range(self.num_layers):
            # <<< ИЗМЕНЕНИЕ: Вызов EncoderLayer с keyword arguments >>>
            x = self.enc_layers[i](x, mask=mask, training=training)
        return x
    def get_config(self): config = super().get_config(); config.update({'num_layers': self.num_layers, 'd_model': self.d_model, 'num_heads': self.num_heads, 'dff': self.dff, 'input_vocab_size': self.input_vocab_size, 'maximum_position_encoding': self.maximum_position_encoding, 'rate': self.rate}); return config

# <<< ИЗМЕНЕНИЕ: Сигнатура call изменена >>>
class Decoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1, **kwargs):
        super().__init__(name='decoder', **kwargs); self.num_layers = num_layers; self.d_model = d_model
        self.num_heads = num_heads; self.dff = dff; self.target_vocab_size = target_vocab_size
        self.maximum_position_encoding = maximum_position_encoding; self.rate = rate
        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model); self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)
        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate, name=f'decoder_layer_{i}') for i in range(num_layers)]; self.dropout = tf.keras.layers.Dropout(rate)
    # <<< ИЗМЕНЕНИЕ: training и маски передаются как keyword arguments >>>
    def call(self, x, enc_output, look_ahead_mask, padding_mask, training=None):
        seq_len = tf.shape(x)[1]; attention_weights = {}; x = self.embedding(x); embedding_scale = tf.math.sqrt(tf.cast(self.d_model, self.compute_dtype));
        x *= embedding_scale; pos_encoding_casted = tf.cast(self.pos_encoding[:, :seq_len, :], self.compute_dtype); x += pos_encoding_casted
        x = self.dropout(x, training=training)
        for i in range(self.num_layers):
            # <<< ИЗМЕНЕНИЕ: Вызов DecoderLayer с keyword arguments >>>
            x, block1, block2 = self.dec_layers[i](x, enc_output,
                                                   look_ahead_mask=look_ahead_mask,
                                                   padding_mask=padding_mask,
                                                   training=training)
            attention_weights[f'decoder_layer{i+1}_block1'] = block1
            attention_weights[f'decoder_layer{i+1}_block2'] = block2
        return x, attention_weights
    def get_config(self): config = super().get_config(); config.update({'num_layers': self.num_layers, 'd_model': self.d_model, 'num_heads': self.num_heads, 'dff': self.dff, 'target_vocab_size': self.target_vocab_size, 'maximum_position_encoding': self.maximum_position_encoding, 'rate': self.rate}); return config

class Transformer(tf.keras.Model):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,
                 target_vocab_size, pe_input, pe_target, rate=0.1, **kwargs):
        super().__init__(**kwargs); self.num_layers = num_layers; self.d_model = d_model; self.num_heads = num_heads
        self.dff = dff; self.input_vocab_size = input_vocab_size; self.target_vocab_size = target_vocab_size
        self.pe_input = pe_input; self.pe_target = pe_target; self.rate = rate
        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)
        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)
        self.final_layer = tf.keras.layers.Dense(target_vocab_size, dtype=tf.float32, name='output_dense')
    # <<< ИЗМЕНЕНИЕ: Вызовы encoder и decoder с keyword arguments >>>
    def call(self, inputs, training=None):
        if isinstance(inputs, (list, tuple)) and len(inputs) == 2: inp, tar = inputs[0], inputs[1]
        else: raise ValueError(f"Unexpected input format for call. Expected tuple/list of 2 tensors, got: {type(inputs)}")
        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar)
        # <<< ИЗМЕНЕНИЕ: Вызов encoder с keyword arguments >>>
        enc_output = self.encoder(inp, mask=enc_padding_mask, training=training)
        # <<< ИЗМЕНЕНИЕ: Вызов decoder с keyword arguments >>>
        dec_output, attention_weights = self.decoder(tar, enc_output,
                                                   look_ahead_mask=combined_mask,
                                                   padding_mask=dec_padding_mask,
                                                   training=training)
        final_output = self.final_layer(dec_output); return final_output
    def get_config(self): config = {'num_layers': self.num_layers, 'd_model': self.d_model, 'num_heads': self.num_heads, 'dff': self.dff, 'input_vocab_size': self.input_vocab_size, 'target_vocab_size': self.target_vocab_size, 'pe_input': self.pe_input, 'pe_target': self.pe_target, 'rate': self.rate}; return config
    @classmethod
    def from_config(cls, config): return cls(**config)
# <<< КОНЕЦ ИСПРАВЛЕННОГО РАЗДЕЛА 3 >>>

# ==============================================================================
# === 4. Функции и Классы для Обучения (Keras API) ============================
# ==============================================================================
print("\nОпределение функций и классов для обучения (Keras API)...")
class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    """Расписание LR с warmup и масштабированием пикового значения."""
    def __init__(self, d_model, warmup_steps=4000, peak_lr_scale=1.0):
        super().__init__()
        self.d_model = tf.cast(d_model, tf.float32)
        self.warmup_steps = float(warmup_steps)
        self.peak_lr_scale = tf.cast(peak_lr_scale, tf.float32)
    def __call__(self, step):
        step = tf.cast(step, tf.float32) + 1.0; arg1 = tf.math.rsqrt(step); arg2 = step * (self.warmup_steps ** -1.5)
        d_model_rsqrt = tf.math.rsqrt(self.d_model + 1e-9); lr = self.peak_lr_scale * d_model_rsqrt * tf.math.minimum(arg1, arg2); return lr
    def get_config(self): return {"d_model": float(self.d_model), "warmup_steps": self.warmup_steps, "peak_lr_scale": float(self.peak_lr_scale)}
loss_object_compile = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
def masked_loss(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0)); loss_ = loss_object_compile(real, pred)
    mask = tf.cast(mask, dtype=loss_.dtype); loss_ *= mask
    return tf.nn.compute_average_loss(loss_, global_batch_size=GLOBAL_BATCH_SIZE)
def masked_accuracy(real, pred):
    pred_ids = tf.cast(tf.argmax(pred, axis=2), dtype=real.dtype); accuracies = tf.equal(real, pred_ids)
    mask = tf.math.logical_not(tf.math.equal(real, 0)); accuracies = tf.math.logical_and(mask, accuracies)
    accuracies = tf.cast(accuracies, dtype=tf.float32); mask = tf.cast(mask, dtype=tf.float32)
    return tf.reduce_sum(accuracies) / (tf.reduce_sum(mask) + 1e-9)

# ==============================================================================
# === 4.5 Callback для удаления старых чекпоинтов =============================
# (Код без изменений)
# ==============================================================================
class DeleteOldCheckpoints(tf.keras.callbacks.Callback):
    """Callback для удаления файла чекпоинта предыдущей эпохи."""
    def __init__(self, checkpoint_dir, filename_pattern):
        super().__init__(); self.checkpoint_dir = checkpoint_dir; self.filename_pattern = filename_pattern
        print(f"DeleteOldCheckpoints инициализирован с шаблоном: {self.filename_pattern}")
    def on_epoch_end(self, epoch, logs=None):
        if epoch > 0:
            previous_epoch_num = epoch
            try:
                previous_filename = self.filename_pattern.format(epoch=previous_epoch_num)
                previous_filepath = os.path.join(self.checkpoint_dir, previous_filename)
                if tf.io.gfile.exists(previous_filepath):
                    print(f"\nУдаление старого чекпоинта: {previous_filepath}")
                    tf.io.gfile.rmtree(previous_filepath)
            except Exception as e: print(f"\nОшибка при удалении старого чекпоинта '{previous_filepath}': {e}")

# ==============================================================================
# === 5. Функции Инференса (Чата) =============================================
# (Код без изменений)
# ==============================================================================
print("\nОпределение функций для чата...")
def decode_sequence(sequence, tokenizer):
    words = []; in_sequence = False
    if not hasattr(tokenizer, 'index_word'): print("Ошибка: Атрибут tokenizer.index_word не найден."); return "[Ошибка декодирования: нет index_word]"
    word_map = tokenizer.index_word
    for idx in sequence:
        idx_int = int(idx);
        if idx_int == 0: continue
        word = word_map.get(idx_int, '<unk>')
        if word == '<start>': in_sequence = True; continue
        if word == '<end>': in_sequence = False; break
        if in_sequence: words.append(word)
    return " ".join(words)
def evaluate_keras(sentence, transformer_inf, tokenizer_inf, start_token_id_inf, end_token_id_inf, max_length=MAX_LENGTH, temperature=1.0):
    if not start_token_id_inf or not end_token_id_inf: return "[Ошибка: ID <start> или <end> токенов не найдены]"
    sentence = preprocess_sentence(sentence); inputs = tokenizer_inf.texts_to_sequences([sentence])
    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_length, padding='post', truncating='post')
    encoder_input = tf.convert_to_tensor(inputs, dtype=tf.int32); decoder_input_ids = [start_token_id_inf]
    output_ids = tf.expand_dims(decoder_input_ids, 0)
    for i in range(max_length):
        model_input_tuple = (encoder_input, output_ids);
        # <<< ИЗМЕНЕНИЕ: Передаем training=False явно >>>
        predictions = transformer_inf(model_input_tuple, training=False)
        predictions = predictions[:, -1:, :]; safe_temperature = tf.maximum(tf.cast(temperature, tf.float32), 1e-9)
        scaled_logits = predictions / safe_temperature; predicted_id = tf.random.categorical(scaled_logits[:, 0, :], num_samples=1, dtype=tf.int32)
        output_ids = tf.concat([output_ids, predicted_id], axis=-1)
        if predicted_id[0][0] == end_token_id_inf: break
    predicted_sequence = output_ids.numpy()[0]; predicted_sentence = decode_sequence(predicted_sequence, tokenizer_inf); return predicted_sentence
def chat_keras(transformer_inf, tokenizer_inf, start_token_id_inf, end_token_id_inf, temperature=DEFAULT_TEMPERATURE):
    print("\n--- Начинаем чат! ---"); print(f"Температура: {temperature}. Введите 'quit' или 'exit', чтобы завершить.")
    while True:
        try: user_input = input("Вы: ")
        except EOFError: print("\nВыход (EOF)."); break
        if user_input.lower() in ['quit', 'exit']: break
        if not user_input.strip(): continue
        start_eval_time = time.time()
        try: response = evaluate_keras(user_input, transformer_inf, tokenizer_inf, start_token_id_inf, end_token_id_inf, temperature=temperature)
        except Exception as e_eval: print(f"\n!!! Ошибка во время evaluate: {e_eval}"); traceback.print_exc(); response = "[Ошибка генерации ответа]"
        eval_time = time.time() - start_eval_time; print("Бот (t={:.2f}, {:.2f}s): {}".format(temperature, eval_time, response))

# ==============================================================================
# === 6. Основной Блок Выполнения =============================================
# ==============================================================================

if __name__ == "__main__":
    print("\n--- Основной блок выполнения ---")

    # --- 6.1 Загрузка и подготовка данных ---
    dataset, tokenizer, vocab_size, token_ids = load_and_prepare_data(
        dataset_name=DATASET_NAME, num_examples=NUM_EXAMPLES, max_length=MAX_LENGTH,
        buffer_size=BUFFER_SIZE, vocab_limit=VOCAB_SIZE_LIMIT, data_dir=DATA_DIR,
        tokenizer_filename=TOKENIZER_FILENAME, global_batch_size=GLOBAL_BATCH_SIZE
    )
    if dataset is None or tokenizer is None or token_ids is None or vocab_size is None:
        print("\nКритическая ошибка при подготовке данных. Выход.")
        exit()
    start_token_id, end_token_id = token_ids
    input_vocab_size = target_vocab_size = vocab_size

    # --- 6.2 Создание/Компиляция/Построение модели и Оптимизатора внутри strategy.scope() --- # <<< Изменено название
    transformer_main = None; optimizer = None
    with strategy.scope():
        print("\nСоздание модели Трансформер и Оптимизатора в scope стратегии...")
        transformer_main = Transformer(
            num_layers=NUM_LAYERS, d_model=D_MODEL, num_heads=NUM_HEADS, dff=DFF,
            input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size,
            pe_input=MAX_LENGTH, pe_target=MAX_LENGTH, rate=DROPOUT_RATE
        )
        print("Модель создана.")

        learning_rate = CustomSchedule(D_MODEL, warmup_steps=WARMUP_STEPS, peak_lr_scale=PEAK_LR_SCALE)
        optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9, clipnorm=CLIPNORM)
        print(f"Оптимизатор Adam с CustomSchedule (peak_scale={PEAK_LR_SCALE}) и clipnorm={CLIPNORM} создан.")

        print("Компиляция модели (явный jit_compile=False для TPU)...")
        transformer_main.compile(
            optimizer=optimizer,
            loss=masked_loss,
            metrics=[masked_accuracy],
            jit_compile=False # Оставляем False для TPU
        )
        print("Модель скомпилирована.")

        # <<< ПЕРЕМЕЩЕНО: Попытка "построить" модель ВНУТРИ scope >>>
        # Это гарантирует, что веса создаются в контексте стратегии
        # Делаем это только если модель не загружена из чекпоинта
        # (latest_checkpoint проверяется позже, но если он есть, модель уже будет построена при загрузке)
        # Поэтому проверяем только transformer_main.built
        if not transformer_main.built:
             print("Попытка построить модель внутри scope...")
             try:
                 example_batch = next(iter(dataset))
                 # Передаем входы и training=False, чтобы инициализировать веса
                 _ = transformer_main(example_batch[0], training=False)
                 print("Модель построена внутри scope.")
                 # Можно вывести summary здесь, если нужно
                 # transformer_main.summary(line_length=120)
             except Exception as e_build:
                 print(f"!!! Ошибка при построении модели внутри scope: {e_build}")
                 print("Продолжаем, но могут быть проблемы...")
        # <<< КОНЕЦ ПЕРЕМЕЩЕННОГО БЛОКА >>>

    # --- 6.3 Настройка Callbacks ---
    # (Без изменений)
    print("\nНастройка Callbacks...")
    checkpoint_dir = os.path.join(DATA_DIR, 'checkpoints_keras_tpu')
    checkpoint_filename_pattern = "ckpt-epoch_{epoch:02d}.keras"
    model_checkpoint_path_template = os.path.join(checkpoint_dir, checkpoint_filename_pattern)
    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
        filepath=model_checkpoint_path_template, save_weights_only=False,
        monitor='masked_accuracy', mode='max', save_best_only=False, save_freq='epoch'
    )
    print(f"ModelCheckpoint настроен для сохранения моделей в: {checkpoint_dir} с шаблоном {checkpoint_filename_pattern}")
    delete_old_callback = DeleteOldCheckpoints(checkpoint_dir=checkpoint_dir, filename_pattern=checkpoint_filename_pattern)
    print("Callback DeleteOldCheckpoints настроен.")
    log_dir = os.path.join(DATA_DIR, "logs_keras_tpu", time.strftime("%Y%m%d-%H%M%S"))
    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
    print(f"TensorBoard логи будут сохраняться в: {log_dir}")
    print(f"Для запуска TensorBoard: tensorboard --logdir {os.path.abspath(log_dir)}")
    callbacks_list = [model_checkpoint_callback, tensorboard_callback, delete_old_callback]

    # --- 6.4 Восстановление из чекпоинта Keras ---
    # (Без изменений)
    print("\nПоиск и восстановление из чекпоинта Keras...")
    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)
    initial_epoch = 0
    if latest_checkpoint:
        print(f"Найден последний чекпоинт Keras: {latest_checkpoint}")
        try:
            print("Попытка загрузки модели из чекпоинта...")
            with strategy.scope():
                 transformer_main = tf.keras.models.load_model(
                     latest_checkpoint,
                     custom_objects={'CustomSchedule': CustomSchedule,
                                     'masked_loss': masked_loss,
                                     'masked_accuracy': masked_accuracy,
                                     'Transformer': Transformer,
                                     'Encoder': Encoder,
                                     'Decoder': Decoder,
                                     'EncoderLayer': EncoderLayer,
                                     'DecoderLayer': DecoderLayer,
                                     'MultiHeadAttention': MultiHeadAttention}
                 )
            print("Модель успешно загружена из Keras чекпоинта.")
            try:
                epoch_str = latest_checkpoint.split('epoch_')[-1].split('.')[0]
                initial_epoch = int(epoch_str)
                print(f"Возобновление обучения с эпохи: {initial_epoch + 1}")
            except (IndexError, ValueError, TypeError) as e_epoch:
                print(f"Не удалось определить номер эпохи из имени чекпоинта ('{latest_checkpoint}'): {e_epoch}. Начинаем с эпохи 0.")
                initial_epoch = 0
        except Exception as e_load:
            print(f"Ошибка загрузки Keras модели из чекпоинта: {e_load}")
            print("Обучение начнется с нуля."); traceback.print_exc(); initial_epoch = 0
            if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir)
    else:
        print("Чекпоинты Keras не найдены, начинаем обучение с нуля."); initial_epoch = 0
        if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir)

    # --- 6.5 Обучение с использованием model.fit() ---
    if EPOCHS > initial_epoch:
        print(f"\n--- Начало обучения Keras model.fit (с эпохи {initial_epoch + 1} до {EPOCHS}) ---")
        try:
            # <<< УДАЛЕНО: Блок построения модели перед fit, т.к. он теперь внутри scope >>>
            # if not latest_checkpoint and not transformer_main.built:
            #      ...

            history = transformer_main.fit(
                dataset, epochs=EPOCHS, initial_epoch=initial_epoch,
                callbacks=callbacks_list, verbose=1
            )
            print("\nОбучение завершено.")
            final_model_path = os.path.join(DATA_DIR, 'transformer_final_keras_tpu_model.keras')
            print(f"\nСохранение финальной модели Keras в {final_model_path}...")
            try:
                transformer_main.save(final_model_path); print("Финальная модель сохранена.")
            except Exception as e_save: print(f"!!! Ошибка при сохранении финальной модели Keras: {e_save}"); traceback.print_exc()
        except Exception as e_fit:
             print(f"\n!!! Ошибка во время выполнения model.fit: {e_fit}"); traceback.print_exc(); print("Обучение прервано.")
    elif EPOCHS == 0:
         print("\nОбучение пропущено (EPOCHS = 0).");
         if not latest_checkpoint: print("ПРЕДУПРЕЖДЕНИЕ: Нет сохраненных моделей/чекпоинтов для использования.")
    else:
         print(f"\nОбучение не требуется. Целевое количество эпох ({EPOCHS}) уже достигнуто ({initial_epoch}).")

    # --- 6.6 Запуск чата ---
    # (Без изменений, блок построения перед чатом остается на всякий случай)
    print("\nПодготовка к запуску чата...")
    if 'transformer_main' in locals() and transformer_main is not None:
         if not transformer_main.built:
             print("Модель не построена, попытка построить перед чатом...")
             try:
                 dummy_q = tf.random.uniform((1, MAX_LENGTH), dtype=tf.int32, maxval=vocab_size)
                 dummy_a_in = tf.random.uniform((1, 1), dtype=tf.int32, maxval=vocab_size)
                 _ = transformer_main((dummy_q, dummy_a_in), training=False)
                 print("Модель построена.")
             except Exception as e_build:
                 print(f"!!! Ошибка при построении модели перед чатом: {e_build}")
                 print("Чат может не работать корректно.")

         if tokenizer and token_ids and start_token_id is not None and end_token_id is not None:
             print("Модель и токенизатор готовы.")
             try: chat_keras(transformer_main, tokenizer, start_token_id, end_token_id, temperature=DEFAULT_TEMPERATURE)
             except Exception as e_chat: print(f"\n!!! Ошибка во время работы чата: {e_chat}"); traceback.print_exc()
         else: print("Ошибка: Токенизатор или ID <start>/<end> токенов не были корректно загружены. Чат невозможен.")
    else: print("Ошибка: Модель не была обучена или загружена из чекпоинта. Чат невозможен.")

    print("\nПрограмма завершена.")