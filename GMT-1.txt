# --- Импорты ---
print("Устанавливаются библиотеки")
# Убедитесь, что вы запускаете эту ячейку один раз за сессию
!pip install datasets numpy pickle5 tensorflow==2.18.0 tensorflow-tpu==2.18.0 --find-links=https://storage.googleapis.com/libtpu-tf-releases/index.html

# --- Импорты ---
import tensorflow as tf
import numpy as np
import time
import os
import re # Для очистки текста в вашем preprocess_sentence
import pickle # Для сохранения/загрузки Keras токенизатора
import traceback # Используется в вашем load_and_prepare_data
from datasets import load_dataset # Используется в вашем load_and_prepare_data

# --- 1. Гиперпараметры ---
print("--- Определение Гиперпараметров ---")

# Параметры модели (~60M)
NUM_LAYERS = 6
D_MODEL = 448
NUM_HEADS = 8
D_FF = D_MODEL * 4 # 1792
DROPOUT_RATE = 0.1
# MAX_POSITION_ENCODING = 2048 # Не используется напрямую в новой модели, т.к. длина ограничена MAX_LENGTH

# Параметры данных и обучения (Адаптированные под ваш код)
DATASET_NAME = "daily_dialog" # Имя датасета для load_dataset
NUM_EXAMPLES = 120000 # Макс. количество пар вопрос-ответ для использования (из вашего кода)
MAX_LENGTH = 40      # Максимальная длина последовательности (токенов) ПОСЛЕ добавления <start>/<end>
BUFFER_SIZE = 20000  # Размер буфера для перемешивания данных
BATCH_SIZE_PER_REPLICA = 64 # Размер батча на одно ядро TPU
EPOCHS = 10          # Количество эпох обучения
VOCAB_LIMIT = 20000  # Лимит словаря для Keras Tokenizer (из вашего кода)
DATA_DIR = "./keras_tokenizer_data" # Папка для Keras токенизатора (из вашего кода)
TOKENIZER_FILENAME = "keras_daily_dialog_tokenizer.pickle" # Имя файла Keras токенизатора

# Параметры сохранения модели
CHECKPOINT_DIR = "./CheckPoints_DailyDialog_KerasTok" # Папка для чекпоинтов
CHECKPOINT_SAVE_FREQ = 5 # Сохранять чекпоинт каждые N эпох
FINAL_MODEL_FILENAME = "final_transformer_model_keras_tok.keras" # Имя файла финальной модели

# Параметры чата
CHAT_TEMPERATURE = 0.7

# Определяем ID паддинга (Keras Tokenizer использует 0 по умолчанию)
PAD_TOKEN_ID = 0
# <start> и <end> ID будут определены после загрузки/создания токенизатора
START_TOKEN_ID = None
END_TOKEN_ID = None
VOCAB_SIZE = None # Будет определен после загрузки/создания токенизатора

# Полные пути
tokenizer_full_path = os.path.join(DATA_DIR, TOKENIZER_FILENAME)
final_model_full_path = os.path.join(CHECKPOINT_DIR, FINAL_MODEL_FILENAME)

print(f"DATASET_NAME: {DATASET_NAME}, NUM_EXAMPLES: {NUM_EXAMPLES}, MAX_LENGTH: {MAX_LENGTH}")
print(f"VOCAB_LIMIT: {VOCAB_LIMIT}, DATA_DIR: {DATA_DIR}")
print(f"NUM_LAYERS: {NUM_LAYERS}, D_MODEL: {D_MODEL}, NUM_HEADS: {NUM_HEADS}, D_FF: {D_FF}")
print(f"BATCH_SIZE_PER_REPLICA: {BATCH_SIZE_PER_REPLICA}, EPOCHS: {EPOCHS}")
print(f"CHECKPOINT_DIR: {CHECKPOINT_DIR}, SAVE_FREQ: {CHECKPOINT_SAVE_FREQ}")
print(f"CHAT_TEMPERATURE: {CHAT_TEMPERATURE}")
print("-" * 30 + "\n")

# Создаем директории, если их нет
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
print(f"Директории {DATA_DIR} и {CHECKPOINT_DIR} проверены/созданы.")


# --- 2. Инициализация TPU ---
print("--- Инициализация TPU ---")
strategy = None
try:
    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')
    print(f"Подключение к TPU: {resolver.master()}")
    tf.config.experimental_connect_to_cluster(resolver)
    tf.tpu.experimental.initialize_tpu_system(resolver)
    strategy = tf.distribute.TPUStrategy(resolver)
    print("TPU инициализирована успешно.")
    print(f"Количество реплик (TPU ядер): {strategy.num_replicas_in_sync}")
except ValueError as e:
    print(f"Ошибка инициализации TPU: {e}. TPU недоступна.")
    print("Продолжение работы без TPU (используя CPU/GPU, если доступны).")
    strategy = tf.distribute.get_strategy()

if strategy is None:
     raise RuntimeError("Не удалось определить стратегию выполнения.")

GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync
print(f"\nИтоговая стратегия: {strategy.__class__.__name__}")
print(f"Итоговое количество реплик: {strategy.num_replicas_in_sync}")
print(f"Глобальный размер батча: {GLOBAL_BATCH_SIZE}")
print(f"Итоговая политика точности: {tf.keras.mixed_precision.global_policy().name}")
print("-" * 30 + "\n")


# ==============================================================================
# === 3. Функции Предобработки Данных (ВАШ КОД) ================================
# ==============================================================================
def preprocess_sentence(w):
    """Очищает предложение и добавляет токены <start>, <end>."""
    if not isinstance(w, str): w = str(w) # Обработка нестроковых данных
    w = w.lower().strip()
    # Добавляем пробелы вокруг пунктуации
    w = re.sub(r"([?.!,])", r" \1 ", w)
    # Заменяем множественные пробелы на один
    w = re.sub(r'[" "]+', " ", w)
    # Убираем пробелы по краям
    w = w.strip()
    # Добавляем стартовый и конечный токены
    w = '<start> ' + w + ' <end>'
    return w

def create_pairs_from_hf_dataset(dataset, num_examples, max_length):
    """Извлекает пары (вопрос, ответ) из датасета Hugging Face."""
    inputs, outputs = [], []
    processed_dialogs = 0
    print("Извлечение пар вопрос-ответ из датасета...")
    total_dialogs = len(dataset)
    skipped_count = 0
    stop_processing = False # Флаг для досрочного выхода

    for example in dataset:
        if stop_processing: break

        processed_dialogs += 1
        # Используем .get() для безопасного доступа к ключу 'dialog'
        dialog = example.get('dialog', [])

        # Убедимся, что dialog это список и содержит хотя бы 2 реплики
        if isinstance(dialog, list) and len(dialog) > 1:
            for i in range(len(dialog) - 1):
                inp_raw = dialog[i]
                out_raw = dialog[i+1]

                # Пропускаем, если реплики пустые или не строки
                if not inp_raw or not out_raw or not isinstance(inp_raw, str) or not isinstance(out_raw, str):
                    skipped_count += 1
                    continue

                inp = preprocess_sentence(inp_raw)
                out = preprocess_sentence(out_raw)

                # Проверяем длину ПОСЛЕ добавления <start>/<end>
                inp_len = len(inp.split())
                out_len = len(out.split())

                # Добавляем пару, если длина подходит (длина > 2, т.к. есть <start>/<end>)
                if inp_len > 2 and inp_len <= max_length and out_len > 2 and out_len <= max_length:
                    inputs.append(inp)
                    outputs.append(out)

                    # Проверяем, достигнут ли лимит пар
                    if len(inputs) >= num_examples:
                        print(f"\nДостигнут лимит в {num_examples} пар.")
                        if skipped_count > 0: print(f"(Пропущено неподходящих пар: {skipped_count})")
                        stop_processing = True # Устанавливаем флаг для выхода из внешнего цикла
                        break # Выходим из внутреннего цикла
                else:
                    skipped_count += 1
        else:
             # Пропускаем диалоги, которые не являются списками или слишком короткие
             skipped_count += 1 # Считаем пропущенный диалог как одну пропущенную "пару" для простоты

        if stop_processing: break # Выходим из внешнего цикла, если лимит достигнут

        # Печать прогресса
        if processed_dialogs % 1000 == 0 or processed_dialogs == total_dialogs:
             print(f"Обработано диалогов: {processed_dialogs}/{total_dialogs}, Собрано пар: {len(inputs)}/{num_examples}, Пропущено: {skipped_count}", end='\r')

    print(f"\nОбработка диалогов завершена. Сгенерировано {len(inputs)} пар.")
    if skipped_count > 0: print(f"(Всего пропущено неподходящих реплик/диалогов/пар: {skipped_count})")
    return inputs, outputs

def load_and_prepare_data(dataset_name, num_examples, max_length, buffer_size, vocab_limit, data_dir, tokenizer_filename, global_batch_size):
    """Загружает данные, создает токенизатор и tf.data.Dataset."""
    print("\n--- Загрузка и обработка данных ---")
    if not os.path.exists(data_dir):
        print(f"Создание директории: {data_dir}")
        os.makedirs(data_dir)

    try:
        print(f"Загрузка '{dataset_name}'...")
        # trust_remote_code=True может быть нужен для некоторых датасетов
        raw_datasets = load_dataset(dataset_name, trust_remote_code=True)
        all_dialogs = []
        # Собираем диалоги из всех доступных сплитов (train, validation, test)
        for split in raw_datasets.keys():
            print(f"Используем диалоги из '{split}' ({len(raw_datasets[split])})")
            all_dialogs.extend(raw_datasets[split])
        print(f"Всего диалогов для обработки: {len(all_dialogs)}")
    except Exception as e:
        print(f"Критическая ошибка при загрузке датасета '{dataset_name}': {e}")
        traceback.print_exc()
        return None, None, None, None

    # Создаем пары вопрос-ответ
    questions, answers = create_pairs_from_hf_dataset(all_dialogs, num_examples, max_length)
    if not questions:
        print("Не удалось создать пары вопрос-ответ.")
        return None, None, None, None

    print(f"Создано {len(questions)} пар.")
    if questions:
        print("Пример пары:")
        print(f"  Вопрос: {questions[0]}")
        print(f"  Ответ: {answers[0]}")

    # Работа с токенизатором Keras
    tokenizer_path = os.path.join(data_dir, tokenizer_filename)
    tokenizer = None
    if os.path.exists(tokenizer_path):
        print(f"Загрузка токенизатора из {tokenizer_path}...")
        try:
            with open(tokenizer_path, 'rb') as handle:
                tokenizer = pickle.load(handle)
            print("Токенизатор загружен.")
        except Exception as e:
            print(f"Ошибка загрузки токенизатора: {e}. Создаем новый.")
            tokenizer = None
    else:
        print("Токенизатор не найден. Создаем новый.")
        tokenizer = None

    if tokenizer is None:
        print("Создание и сохранение нового токенизатора Keras...")
        # num_words = vocab_limit + 1? Нет, num_words - это макс. индекс, 0 зарезервирован.
        # filters='' чтобы Keras не удалял пунктуацию, т.к. мы ее обработали
        tokenizer = tf.keras.preprocessing.text.Tokenizer(
            num_words=vocab_limit if vocab_limit else None, # Ограничение словаря
            oov_token='<unk>', # Токен для неизвестных слов
            filters='' # Не применяем стандартные фильтры
        )
        # Обучаем токенизатор на всех текстах
        tokenizer.fit_on_texts(questions + answers)
        try:
            with open(tokenizer_path, 'wb') as handle:
                pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
            print(f"Токенизатор сохранен в {tokenizer_path}")
        except Exception as e:
            print(f"Ошибка сохранения токенизатора: {e}")

    # Определяем фактический размер словаря
    # +1, т.к. индексы начинаются с 1, а 0 зарезервирован для паддинга
    actual_vocab_size = len(tokenizer.word_index) + 1
    # Используем либо лимит (+1 для паддинга), либо фактический размер, если он меньше лимита
    vocab_size = min(vocab_limit + 1, actual_vocab_size) if vocab_limit else actual_vocab_size
    print(f"Размер словаря (с OOV и паддингом 0): {vocab_size}")

    # Получаем ID <start> и <end> токенов
    start_token_id = tokenizer.word_index.get('<start>')
    end_token_id = tokenizer.word_index.get('<end>')
    if start_token_id is None or end_token_id is None:
        print("Критическая ошибка: не найдены <start> или <end> токены в словаре!")
        return None, tokenizer, vocab_size, None
    print(f"ID <start>: {start_token_id}, ID <end>: {end_token_id}")

    # Преобразуем тексты в последовательности ID
    print("Преобразование в последовательности и паддинг...")
    try:
        question_seqs = tokenizer.texts_to_sequences(questions)
        answer_seqs = tokenizer.texts_to_sequences(answers)
    except Exception as e:
        print(f"Ошибка токенизации текстов: {e}")
        return None, tokenizer, vocab_size, (start_token_id, end_token_id)

    # Паддинг последовательностей
    question_seqs = tf.keras.preprocessing.sequence.pad_sequences(
        question_seqs, maxlen=max_length, padding='post', truncating='post'
    )
    answer_seqs = tf.keras.preprocessing.sequence.pad_sequences(
        answer_seqs, maxlen=max_length, padding='post', truncating='post'
    )
    print(f"Форма вопросов после паддинга: {question_seqs.shape}")
    print(f"Форма ответов после паддинга: {answer_seqs.shape}")

    if question_seqs.shape[0] == 0:
        print("Ошибка: Нет данных для создания Dataset после обработки.")
        return None, tokenizer, vocab_size, (start_token_id, end_token_id)

    # Создаем входы и выходы для модели
    # Encoder Input: Полные последовательности вопросов
    encoder_inputs = tf.cast(question_seqs, tf.int32)
    # Decoder Input: Последовательности ответов без последнего токена (<end>)
    decoder_inputs = tf.cast(answer_seqs[:, :-1], tf.int32)
    # Decoder Output (Target): Последовательности ответов без первого токена (<start>)
    decoder_outputs = tf.cast(answer_seqs[:, 1:], tf.int32)

    print(f"Форма входа энкодера: {encoder_inputs.shape}")
    print(f"Форма входа декодера: {decoder_inputs.shape}")
    print(f"Форма выхода декодера (метки): {decoder_outputs.shape}")

    # Создаем tf.data.Dataset
    # Формат: ((вход_энкодера, вход_декодера), выход_декодера)
    dataset = tf.data.Dataset.from_tensor_slices(
        ((encoder_inputs, decoder_inputs), decoder_outputs)
    )
    dataset = dataset.cache() # Кэшируем для скорости
    dataset = dataset.shuffle(buffer_size)
    dataset = dataset.batch(global_batch_size)
    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) # Оптимизация prefetch
    print("tf.data.Dataset создан и оптимизирован.")

    # Печать примера из датасета
    print("\nПример обработанных данных (первый элемент первого батча):")
    try:
        for (enc_in_batch, dec_in_batch), dec_out_batch in dataset.take(1):
            print(f"  Структура батча: ((enc_in, dec_in), dec_out)")
            print(f"  Encoder Input IDs shape: {enc_in_batch.shape}")
            print(f"  Decoder Input IDs shape: {dec_in_batch.shape}")
            print(f"  Decoder Target IDs shape: {dec_out_batch.shape}")
            enc_in_sample = enc_in_batch[0].numpy()
            dec_in_sample = dec_in_batch[0].numpy()
            dec_out_sample = dec_out_batch[0].numpy()

            # Функция для декодирования (использует Keras tokenizer)
            def _decode(seq):
                 if hasattr(tokenizer, 'index_word'):
                     # Игнорируем паддинг (ID=0)
                     return " ".join([tokenizer.index_word.get(i, '?') for i in seq if i != 0])
                 else:
                     return "[Ошибка: tokenizer.index_word не найден]"

            print(f"  Encoder Input Text (sample): {_decode(enc_in_sample)}")
            print(f"  Decoder Input Text (sample): {_decode(dec_in_sample)}")
            print(f"  Decoder Target Text (sample): {_decode(dec_out_sample)}")
            break # Показываем только первый батч
    except Exception as e:
        print(f"Не удалось получить пример батча: {e}")

    return dataset, tokenizer, vocab_size, (start_token_id, end_token_id)
# ==============================================================================
# === / Конец Блока Предобработки Данных =======================================
# ==============================================================================


# --- Запуск подготовки данных ---
print("Запуск load_and_prepare_data...")
train_dataset, tokenizer, VOCAB_SIZE, token_ids = load_and_prepare_data(
    dataset_name=DATASET_NAME,
    num_examples=NUM_EXAMPLES,
    max_length=MAX_LENGTH,
    buffer_size=BUFFER_SIZE,
    vocab_limit=VOCAB_LIMIT,
    data_dir=DATA_DIR,
    tokenizer_filename=TOKENIZER_FILENAME,
    global_batch_size=GLOBAL_BATCH_SIZE
)

# Проверяем результат
if train_dataset is None or tokenizer is None or VOCAB_SIZE is None or token_ids is None:
    raise RuntimeError("Ошибка во время подготовки данных. Невозможно продолжить.")
else:
    START_TOKEN_ID, END_TOKEN_ID = token_ids
    print("\nПодготовка данных успешно завершена.")
    print(f"Итоговый размер словаря: {VOCAB_SIZE}")
    print(f"ID <start>: {START_TOKEN_ID}, ID <end>: {END_TOKEN_ID}, ID <pad>: {PAD_TOKEN_ID}")
    print("-" * 30 + "\n")


# --- 4. Архитектура Модели Transformer ---
print("--- Определение Архитектуры Модели ---")

# --- Позиционное кодирование ---
def get_angles(pos, i, d_model):
    """Вычисляет углы для позиционного кодирования."""
    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
    return pos * angle_rates

def positional_encoding(position, d_model):
    """
    Генерирует тензор позиционного кодирования.

    Args:
        position: Максимальная длина последовательности.
        d_model: Размерность модели.

    Returns:
        tf.Tensor формы (1, position, d_model)
    """
    angle_rads = get_angles(np.arange(position)[:, np.newaxis],
                            np.arange(d_model)[np.newaxis, :],
                            d_model)
    # Применяем sin к четным индексам (2i)
    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
    # Применяем cos к нечетным индексам (2i+1)
    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])

    pos_encoding = angle_rads[np.newaxis, ...] # Добавляем batch dimension

    return tf.cast(pos_encoding, dtype=tf.float32)


# --- Маскирование ---
def create_padding_mask(seq):
    """
    Создает маску для паддинг-токенов во входной последовательности.
    Маскируются позиции, где seq == PAD_TOKEN_ID.

    Args:
        seq: Тензор последовательности формы (batch_size, seq_len).

    Returns:
        tf.Tensor формы (batch_size, 1, 1, seq_len)
    """
    seq = tf.cast(tf.math.equal(seq, PAD_TOKEN_ID), tf.float32)
    # Добавляем измерения для broadcasting в attention: (batch_size, 1, 1, seq_len)
    return seq[:, tf.newaxis, tf.newaxis, :]

def create_look_ahead_mask(size):
    """
    Создает маску для предотвращения заглядывания вперед в self-attention декодера.
    Маскируются будущие позиции.

    Args:
        size: Длина последовательности.

    Returns:
        tf.Tensor формы (size, size)
    """
    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
    return mask # Shape: (size, size)

def create_masks(inp, tar):
    """
    Создает все необходимые маски для энкодера и декодера.

    Args:
        inp: Входная последовательность энкодера (batch_size, inp_seq_len).
        tar: Входная последовательность декодера (batch_size, tar_seq_len).

    Returns:
        Кортеж из трех масок:
        - enc_padding_mask: Маска паддинга для энкодера.
        - combined_mask: Комбинированная маска (look-ahead + padding) для self-attention декодера.
        - dec_padding_mask: Маска паддинга для cross-attention декодера (маскирует паддинг в inp).
    """
    # Маска паддинга для энкодера (маскирует паддинг в inp)
    enc_padding_mask = create_padding_mask(inp)

    # Маска паддинга для декодера (используется в cross-attention)
    # Маскирует паддинг во входах энкодера (key, value из enc_output)
    dec_padding_mask = create_padding_mask(inp)

    # Маска для self-attention декодера (первый блок внимания)
    # 1. Маскирует будущие токены (look-ahead)
    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])
    # 2. Маскирует паддинг в самом таргете (tar)
    dec_target_padding_mask = create_padding_mask(tar)
    # 3. Объединяем обе маски: маскируем и паддинг, и будущие токены
    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)

    return enc_padding_mask, combined_mask, dec_padding_mask


# --- Feed Forward Network ---
def point_wise_feed_forward_network(d_model, d_ff):
    """
    Создает Feed Forward Network (Position-wise).
    Состоит из двух Dense слоев с ReLU активацией между ними.

    Args:
        d_model: Размерность входа/выхода.
        d_ff: Размерность внутреннего слоя.

    Returns:
        Объект tf.keras.Sequential.
    """
    return tf.keras.Sequential([
        tf.keras.layers.Dense(d_ff, activation='relu'), # (batch_size, seq_len, d_ff)
        tf.keras.layers.Dense(d_model)                # (batch_size, seq_len, d_model)
    ], name="ffn")


# --- Слой Энкодера (Encoder Layer) ---
class EncoderLayer(tf.keras.layers.Layer):
    """Один слой энкодера, состоящий из Multi-Head Attention и Feed Forward Network."""
    def __init__(self, d_model, num_heads, d_ff, rate=0.1, name="encoder_layer", **kwargs):
        super(EncoderLayer, self).__init__(name=name, **kwargs)
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_ff = d_ff
        self.rate = rate

        # Проверяем делимость d_model на num_heads для key_dim
        if d_model % num_heads != 0:
            raise ValueError(f"d_model ({d_model}) должно быть делимо на num_heads ({num_heads})")
        key_dim = d_model // num_heads

        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, dropout=rate, name="mha")
        self.ffn = point_wise_feed_forward_network(d_model, d_ff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name="ln_1")
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name="ln_2")

        self.dropout1 = tf.keras.layers.Dropout(rate, name="dropout_1")
        self.dropout2 = tf.keras.layers.Dropout(rate, name="dropout_2")

    def call(self, x, training, mask):
        """
        Прямой проход слоя энкодера.

        Args:
            x: Входной тензор (batch_size, input_seq_len, d_model).
            training: Флаг (bool), указывающий на режим обучения (для dropout).
            mask: Маска паддинга для входа (batch_size, 1, 1, input_seq_len).

        Returns:
            Выходной тензор (batch_size, input_seq_len, d_model).
        """
        # Multi-Head Attention (self-attention)
        # Keras MHA ожидает маску формы (batch_size, ..., seq_len_q, seq_len_k)
        # Наша padding_mask (batch_size, 1, 1, seq_len) подходит для broadcast
        attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask, training=training)

        # Dropout и Add & Norm
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output) # Residual connection

        # Feed Forward Network
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output) # Residual connection

        return out2

    def get_config(self):
        """Возвращает конфигурацию слоя для сериализации."""
        config = super().get_config()
        config.update({
            'd_model': self.d_model,
            'num_heads': self.num_heads,
            'd_ff': self.d_ff,
            'rate': self.rate,
        })
        return config


# --- Слой Декодера (Decoder Layer) ---
class DecoderLayer(tf.keras.layers.Layer):
    """Один слой декодера, состоящий из Masked MHA, Cross MHA и FFN."""
    def __init__(self, d_model, num_heads, d_ff, rate=0.1, name="decoder_layer", **kwargs):
        super(DecoderLayer, self).__init__(name=name, **kwargs)
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_ff = d_ff
        self.rate = rate

        if d_model % num_heads != 0:
            raise ValueError(f"d_model ({d_model}) должно быть делимо на num_heads ({num_heads})")
        key_dim = d_model // num_heads

        # Первый блок внимания: Masked Multi-Head Attention (self-attention)
        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, dropout=rate, name="masked_mha")
        # Второй блок внимания: Multi-Head Attention (cross-attention)
        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, dropout=rate, name="cross_mha")

        self.ffn = point_wise_feed_forward_network(d_model, d_ff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name="ln_1")
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name="ln_2")
        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name="ln_3")

        self.dropout1 = tf.keras.layers.Dropout(rate, name="dropout_1")
        self.dropout2 = tf.keras.layers.Dropout(rate, name="dropout_2")
        self.dropout3 = tf.keras.layers.Dropout(rate, name="dropout_3")


    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
        """
        Прямой проход слоя декодера.

        Args:
            x: Входной тензор декодера (batch_size, target_seq_len, d_model).
            enc_output: Выходной тензор энкодера (batch_size, input_seq_len, d_model).
            training: Флаг режима обучения (bool).
            look_ahead_mask: Комбинированная маска для self-attention (batch_size, 1, target_seq_len, target_seq_len).
            padding_mask: Маска паддинга для cross-attention (batch_size, 1, 1, input_seq_len).

        Returns:
            Выходной тензор (batch_size, target_seq_len, d_model).
        """
        # 1. Masked Multi-Head Attention (self-attention)
        # Query, Key, Value - из входа декодера 'x'
        # Маска: look_ahead_mask (предотвращает заглядывание вперед и маскирует паддинг в 'x')
        attn1 = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training)
        attn1 = self.dropout1(attn1, training=training)
        out1 = self.layernorm1(attn1 + x) # Residual connection

        # 2. Multi-Head Attention (cross-attention)
        # Query - из выхода первого блока внимания (out1)
        # Key, Value - из выхода энкодера (enc_output)
        # Маска: padding_mask (маскирует паддинг в enc_output)
        attn2 = self.mha2(query=out1, value=enc_output, key=enc_output, attention_mask=padding_mask, training=training)
        attn2 = self.dropout2(attn2, training=training)
        out2 = self.layernorm2(attn2 + out1) # Residual connection

        # 3. Feed Forward Network
        ffn_output = self.ffn(out2)
        ffn_output = self.dropout3(ffn_output, training=training)
        out3 = self.layernorm3(ffn_output + out2) # Residual connection

        return out3

    def get_config(self):
        """Возвращает конфигурацию слоя для сериализации."""
        config = super().get_config()
        config.update({
            'd_model': self.d_model,
            'num_heads': self.num_heads,
            'd_ff': self.d_ff,
            'rate': self.rate,
        })
        return config


# --- Энкодер ---
class Encoder(tf.keras.layers.Layer):
    """Полный энкодер, состоящий из Embedding, Positional Encoding и N слоев EncoderLayer."""
    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size,
                 max_seq_len, rate=0.1, name="encoder", **kwargs):
        super(Encoder, self).__init__(name=name, **kwargs)
        self.num_layers = num_layers
        self.d_model = d_model
        self.rate = rate

        # Слои
        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model, name="embedding")
        # Генерируем pos encoding для максимальной длины
        self.pos_encoding = positional_encoding(max_seq_len, d_model)
        self.enc_layers = [EncoderLayer(d_model, num_heads, d_ff, rate, name=f'layer_{i}')
                           for i in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(rate, name="dropout")

    def call(self, x, training, mask):
        """
        Прямой проход энкодера.

        Args:
            x: Входная последовательность ID токенов (batch_size, input_seq_len).
            training: Флаг режима обучения (bool).
            mask: Маска паддинга для входа (batch_size, 1, 1, input_seq_len).

        Returns:
            Выходной тензор энкодера (batch_size, input_seq_len, d_model).
        """
        seq_len = tf.shape(x)[1] # Фактическая длина последовательности в батче

        # 1. Embedding + Positional Encoding
        x = self.embedding(x) # (batch_size, seq_len, d_model)
        # Масштабируем эмбеддинги (как в оригинальной статье)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        # Добавляем позиционное кодирование, обрезая его до фактической длины seq_len
        x += self.pos_encoding[:, :seq_len, :]

        # 2. Dropout
        x = self.dropout(x, training=training)

        # 3. Пропускаем через N слоев EncoderLayer
        for i in range(self.num_layers):
            x = self.enc_layers[i](x, training=training, mask=mask) # Передаем training и mask

        return x # (batch_size, input_seq_len, d_model)

    def get_config(self):
        """Возвращает конфигурацию энкодера для сериализации."""
        config = super().get_config()
        # Сохраняем параметры, необходимые для восстановления
        inner_layer = self.enc_layers[0] if self.enc_layers else None
        config.update({
            'num_layers': self.num_layers,
            'd_model': self.d_model,
            'num_heads': inner_layer.num_heads if inner_layer else None, # Берем из слоя
            'd_ff': inner_layer.d_ff if inner_layer else None,           # Берем из слоя
            'input_vocab_size': self.embedding.input_dim,
            'max_seq_len': self.pos_encoding.shape[1], # Сохраняем размер pos encoding
            'rate': self.rate
        })
        return config


# --- Декодер ---
class Decoder(tf.keras.layers.Layer):
    """Полный декодер, состоящий из Embedding, Positional Encoding и N слоев DecoderLayer."""
    def __init__(self, num_layers, d_model, num_heads, d_ff, target_vocab_size,
                 max_seq_len, rate=0.1, name="decoder", **kwargs):
        super(Decoder, self).__init__(name=name, **kwargs)
        self.num_layers = num_layers
        self.d_model = d_model
        self.rate = rate

        # Слои
        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model, name="embedding")
        self.pos_encoding = positional_encoding(max_seq_len, d_model)
        self.dec_layers = [DecoderLayer(d_model, num_heads, d_ff, rate, name=f'layer_{i}')
                           for i in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(rate, name="dropout")

    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
        """
        Прямой проход декодера.

        Args:
            x: Входная последовательность ID токенов декодера (batch_size, target_seq_len).
            enc_output: Выходной тензор энкодера (batch_size, input_seq_len, d_model).
            training: Флаг режима обучения (bool).
            look_ahead_mask: Комбинированная маска для self-attention декодера.
            padding_mask: Маска паддинга для cross-attention декодера.

        Returns:
            Выходной тензор декодера (batch_size, target_seq_len, d_model).
        """
        seq_len = tf.shape(x)[1]

        # 1. Embedding + Positional Encoding
        x = self.embedding(x) # (batch_size, target_seq_len, d_model)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]

        # 2. Dropout
        x = self.dropout(x, training=training)

        # 3. Пропускаем через N слоев DecoderLayer
        for i in range(self.num_layers):
            # Передаем все необходимые аргументы с ключевыми словами
            x = self.dec_layers[i](x, enc_output,
                                   training=training,
                                   look_ahead_mask=look_ahead_mask,
                                   padding_mask=padding_mask)

        return x # (batch_size, target_seq_len, d_model)

    def get_config(self):
        """Возвращает конфигурацию декодера для сериализации."""
        config = super().get_config()
        inner_layer = self.dec_layers[0] if self.dec_layers else None
        config.update({
            'num_layers': self.num_layers,
            'd_model': self.d_model,
            'num_heads': inner_layer.num_heads if inner_layer else None,
            'd_ff': inner_layer.d_ff if inner_layer else None,
            'target_vocab_size': self.embedding.input_dim,
            'max_seq_len': self.pos_encoding.shape[1],
            'rate': self.rate
        })
        return config


# --- Собираем Трансформер ---
class Transformer(tf.keras.Model):
    """Полная модель Трансформера (Encoder-Decoder)."""
    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size,
                 target_vocab_size, max_seq_len, rate=0.1, name="transformer", **kwargs):
        super(Transformer, self).__init__(name=name, **kwargs)
        # Сохраняем основные параметры
        self.num_layers = num_layers
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_ff = d_ff
        self.rate = rate
        self.max_seq_len = max_seq_len # Сохраняем max_seq_len

        # Создаем энкодер и декодер
        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff,
                               input_vocab_size, max_seq_len, rate)
        self.decoder = Decoder(num_layers, d_model, num_heads, d_ff,
                               target_vocab_size, max_seq_len, rate)

        # Финальный линейный слой для получения логитов словаря
        self.final_layer = tf.keras.layers.Dense(target_vocab_size, name="output_dense")

    def call(self, inputs, training=None):
        """
        Прямой проход модели Трансформер.

        Args:
            inputs: Кортеж из двух тензоров: (inp, tar)
                    - inp: Вход энкодера (batch_size, inp_seq_len).
                    - tar: Вход декодера (сдвинутый таргет) (batch_size, tar_seq_len).
            training: Флаг режима обучения (bool).

        Returns:
            Логиты выходного словаря (batch_size, tar_seq_len, target_vocab_size).
        """
        # Распаковываем входы
        inp, tar = inputs

        # Создаем маски внутри call для корректной работы с @tf.function и TPU
        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar)

        # Прогоняем через энкодер (используем КЛЮЧЕВЫЕ аргументы)
        # enc_output shape: (batch_size, inp_seq_len, d_model)
        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)

        # Прогоняем через декодер (используем КЛЮЧЕВЫЕ аргументы)
        # dec_output shape: (batch_size, tar_seq_len, d_model)
        dec_output = self.decoder(tar, enc_output, training=training,
                                  look_ahead_mask=combined_mask,
                                  padding_mask=dec_padding_mask)

        # Прогоняем через финальный слой
        # final_output shape: (batch_size, tar_seq_len, target_vocab_size)
        final_output = self.final_layer(dec_output)

        return final_output

    def get_config(self):
        """Возвращает конфигурацию модели для сериализации."""
        config = super(Transformer, self).get_config()
        # Добавляем параметры, необходимые для __init__
        config.update({
            'num_layers': self.num_layers,
            'd_model': self.d_model,
            'num_heads': self.num_heads,
            'd_ff': self.d_ff,
            'input_vocab_size': self.encoder.embedding.input_dim, # Получаем из слоя
            'target_vocab_size': self.decoder.embedding.input_dim, # Получаем из слоя
            'max_seq_len': self.max_seq_len, # Используем сохраненное значение
            'rate': self.rate
        })
        return config

    @classmethod
    def from_config(cls, config):
         """Создает модель из конфигурации (для загрузки)."""
         # Keras автоматически восстановит вложенные слои (encoder, decoder, final_layer)
         # по их сохраненным конфигурациям, если они стандартные или тоже имеют from_config.
         # Мы просто передаем параметры в __init__.
         return cls(**config)

print("Архитектура модели определена.")
print("-" * 30 + "\n")


# --- 5. Оптимизатор и Функция Потерь ---
print("--- Определение Оптимизатора и Функции Потерь ---")

# Кастомное расписание Learning Rate (без изменений)
class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, d_model, warmup_steps=4000):
        super(CustomSchedule, self).__init__()
        self.d_model = tf.cast(d_model, tf.float32)
        self.warmup_steps = warmup_steps
    def __call__(self, step):
        step = tf.cast(step, tf.float32)
        arg1 = tf.math.rsqrt(tf.maximum(step, 1.0)) # Избегаем rsqrt(0)
        arg2 = step * (self.warmup_steps ** -1.5)
        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)
    def get_config(self):
        return {"d_model": float(self.d_model), "warmup_steps": self.warmup_steps}

# Функция потерь и метрика (Адаптированы под PAD_TOKEN_ID = 0)
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')

def masked_loss(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, PAD_TOKEN_ID)) # Используем PAD_TOKEN_ID = 0
    loss_ = loss_object(real, pred)
    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask
    return tf.reduce_sum(loss_) / (tf.reduce_sum(mask) + 1e-8) # Добавляем epsilon

def masked_accuracy(real, pred):
    accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), dtype=real.dtype))
    mask = tf.math.logical_not(tf.math.equal(real, PAD_TOKEN_ID)) # Используем PAD_TOKEN_ID = 0
    accuracies = tf.logical_and(mask, accuracies)
    accuracies = tf.cast(accuracies, dtype=tf.float32)
    mask = tf.cast(mask, dtype=tf.float32)
    sum_mask = tf.reduce_sum(mask)
    return tf.where(sum_mask > 0, tf.reduce_sum(accuracies) / sum_mask, 0.0) # Избегаем деления на 0

print("Оптимизатор и функции потерь/метрик определены.")
print("-" * 30 + "\n")


# --- 6. Обучение ---
print("--- Настройка Обучения ---")
transformer_model = None # Ссылка на модель для сохранения

with strategy.scope():
    print("Создание модели и оптимизатора в strategy.scope()...")
    # Создание модели
    transformer = Transformer(
        num_layers=NUM_LAYERS, d_model=D_MODEL, num_heads=NUM_HEADS, d_ff=D_FF,
        input_vocab_size=VOCAB_SIZE, target_vocab_size=VOCAB_SIZE,
        max_seq_len=MAX_LENGTH, rate=DROPOUT_RATE
    )
    transformer_model = transformer

    # Создание оптимизатора
    learning_rate = CustomSchedule(D_MODEL)
    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)
    print("Модель и оптимизатор созданы.")

    # --- Вывод информации о модели ---
    print("\n--- Параметры Модели ---")
    # ... (таблица параметров без изменений) ...
    print(f"| {'Параметр':<30} | {'Значение':<20} |")
    print(f"|{'-'*32}|{'-'*22}|")
    print(f"| {'Количество слоев (Encoder/Decoder)':<30} | {NUM_LAYERS:<20} |")
    print(f"| {'Размерность модели (d_model)':<30} | {D_MODEL:<20} |")
    print(f"| {'Количество голов внимания':<30} | {NUM_HEADS:<20} |")
    print(f"| {'Размерность FFN (d_ff)':<30} | {D_FF:<20} |")
    print(f"| {'Размер словаря (Vocab Size)':<30} | {VOCAB_SIZE:<20} |")
    print(f"| {'Макс. длина посл. (MAX_LENGTH)':<30} | {MAX_LENGTH:<20} |")
    print(f"| {'Dropout Rate':<30} | {DROPOUT_RATE:<20} |")
    print(f"|{'-'*32}|{'-'*22}|")


    # --- Model Summary ---
    print("\n--- Model Summary ---")
    # ... (код model summary без изменений) ...
    try:
         dummy_enc_inp = tf.keras.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name="encoder_input")
         dummy_dec_inp = tf.keras.Input(shape=(MAX_LENGTH-1,), dtype=tf.int32, name="decoder_input")
         model_for_summary = tf.keras.Model(inputs=(dummy_enc_inp, dummy_dec_inp), outputs=transformer((dummy_enc_inp, dummy_dec_inp)))
         model_for_summary.summary(line_length=120)
         total_params = model_for_summary.count_params()
         print(f"\nОбщее количество параметров: {total_params:,}")
         print(f"Целевое количество (~60M): {60_000_000:,}")
         if abs(total_params - 60_000_000) / 60_000_000 < 0.1:
             print("Количество параметров близко к целевому.")
         else:
             print("ВНИМАНИЕ: Количество параметров отличается от целевого.")
    except Exception as e:
        print(f"\nНе удалось вывести model.summary(): {e}")
        traceback.print_exc()
        print("Продолжение без summary.")
    print("-" * 30 + "\n")


    # --- Чекпоинты ---
    print(f"Настройка чекпоинтов в директории: {CHECKPOINT_DIR}...")
    ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)
    ckpt_manager = tf.train.CheckpointManager(ckpt, CHECKPOINT_DIR, max_to_keep=5)
    if ckpt_manager.latest_checkpoint:
        ckpt.restore(ckpt_manager.latest_checkpoint)
        print(f"Восстановлен последний чекпоинт: {ckpt_manager.latest_checkpoint}")
        print(f"Восстановлен шаг оптимизатора: {optimizer.iterations.numpy()}")
    else:
        print("Чекпоинт не найден, инициализация с нуля.")
    print("-" * 30 + "\n")

# --- Определяем функции train_step / distributed_train_step ---
# Важно: эти функции должны использовать метрики, определенные *внутри* цикла
# Мы не можем определить их здесь, т.к. они будут пересоздаваться в цикле.
# Вместо этого передадим их как аргументы или будем обращаться к ним по имени.
# Проще всего оставить их как есть, они будут использовать переменные из внешней области видимости,
# которые будут обновляться в цикле.

@tf.function
def train_step(inputs, current_train_loss, current_train_accuracy): # Принимаем метрики как аргументы
    (enc_in, dec_in), dec_out = inputs
    with tf.GradientTape() as tape:
        predictions = transformer((enc_in, dec_in), training=True) # Используем transformer, созданный в scope
        loss = masked_loss(dec_out, predictions)
    gradients = tape.gradient(loss, transformer.trainable_variables)
    trainable_vars = transformer.trainable_variables
    grads_and_vars = [(g, v) for g, v in zip(gradients, trainable_vars) if g is not None]
    if len(grads_and_vars) != len(trainable_vars):
         tf.print("Warning: Some gradients are None.")
    optimizer.apply_gradients(grads_and_vars) # Используем optimizer, созданный в scope

    # Обновляем переданные метрики
    current_train_loss.update_state(loss)
    current_train_accuracy.update_state(masked_accuracy(dec_out, predictions))
    return loss

@tf.function
def distributed_train_step(dataset_inputs, loss_metric, acc_metric): # Принимаем метрики
    # strategy.run теперь нужно передать метрики в train_step
    # Это немного сложнее, т.к. strategy.run ожидает только dataset args
    # Проще оставить как было, полагаясь на область видимости Python,
    # что train_step увидит метрики, созданные в цикле эпохи.
    # Убедимся, что train_step НЕ принимает метрики как аргументы.

    per_replica_losses = strategy.run(train_step_wrapper, args=(dataset_inputs,)) # Используем обертку
    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)

# --- Цикл Обучения ---
print("--- Начало Обучения ---")
start_time_total = time.time()
initial_epoch = 0

for epoch in range(initial_epoch, EPOCHS):
    start_time_epoch = time.time()

    # --- Пересоздаем метрики в начале каждой эпохи ---
    print(f"\n[INFO] Пересоздание метрик для Эпохи {epoch+1}")
    # Определяем их здесь, чтобы они были доступны train_step через замыкание
    train_loss = tf.keras.metrics.Mean(name='train_loss')
    train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')
    print(f"[DEBUG] Тип train_loss после пересоздания: {type(train_loss)}")
    # reset_states() теперь не нужен

    # Обертка для train_step, чтобы она видела метрики текущей эпохи
    # Это нужно, чтобы избежать передачи метрик через strategy.run
    @tf.function
    def train_step_wrapper(inputs):
        (enc_in, dec_in), dec_out = inputs
        with tf.GradientTape() as tape:
            predictions = transformer((enc_in, dec_in), training=True)
            loss = masked_loss(dec_out, predictions)
        gradients = tape.gradient(loss, transformer.trainable_variables)
        trainable_vars = transformer.trainable_variables
        grads_and_vars = [(g, v) for g, v in zip(gradients, trainable_vars) if g is not None]
        if len(grads_and_vars) != len(trainable_vars):
             tf.print("Warning: Some gradients are None.")
        optimizer.apply_gradients(grads_and_vars)

        # Обновляем метрики из внешней области видимости (цикла эпохи)
        train_loss.update_state(loss)
        train_accuracy.update_state(masked_accuracy(dec_out, predictions))
        return loss


    distributed_train_dataset = strategy.experimental_distribute_dataset(train_dataset)

    for batch_num, batch in enumerate(distributed_train_dataset):
        # Вызываем distributed_train_step, который вызовет train_step_wrapper
        batch_loss = distributed_train_step(batch, train_loss, train_accuracy) # Передаем метрики

        if batch_num % 50 == 0:
            current_lr = learning_rate(tf.cast(optimizer.iterations, tf.float32))
            # Читаем результат только что созданных и обновленных метрик
            print(f'Эпоха {epoch + 1}/{EPOCHS} Батч {batch_num} '
                  f'Loss: {train_loss.result():.4f} '
                  f'Accuracy: {train_accuracy.result():.4f} '
                  f'LR: {current_lr:.6f}')

    # Сохранение чекпоинта
    if (epoch + 1) % CHECKPOINT_SAVE_FREQ == 0 or (epoch + 1) == EPOCHS:
        ckpt_save_path = ckpt_manager.save()
        print(f'Сохранен чекпоинт для эпохи {epoch+1} в {ckpt_save_path}')
    else:
         print(f'Эпоха {epoch + 1}: Пропуск сохранения чекпоинта.')

    epoch_time = time.time() - start_time_epoch
    # Читаем итоговые результаты метрик за эпоху
    print(f'\nЭпоха {epoch + 1} Завершена.')
    print(f'  Loss: {train_loss.result():.4f}')
    print(f'  Accuracy: {train_accuracy.result():.4f}')
    print(f'  Время на эпоху: {epoch_time:.2f} сек')
    print("-" * 30)

# --- Завершение обучения и сохранение ---
total_time = time.time() - start_time_total
print(f"--- Обучение Завершено ---")
print(f"Общее время обучения: {total_time:.2f} сек ({total_time/60:.2f} мин)")

if transformer_model is not None:
    print(f"\nСохранение финальной модели в {final_model_full_path}...")
    transformer_model.save(final_model_full_path, save_format='keras')
    print("Финальная модель сохранена.")
else:
    print("Ошибка: Не удалось получить ссылку на модель для сохранения.")


# --- 7. Интерактивный Чат ---
# (Код чата остается без изменений)
print("\n--- Запуск Интерактивного Чата ---")
# ... (код загрузки модели, токенизатора и цикла чата) ...
if os.path.exists(final_model_full_path):
    print(f"Загрузка финальной модели из {final_model_full_path}...")
    # Регистрируем кастомные объекты (без изменений)
    custom_objects = {
        'Transformer': Transformer, 'Encoder': Encoder, 'Decoder': Decoder,
        'EncoderLayer': EncoderLayer, 'DecoderLayer': DecoderLayer,
        'CustomSchedule': CustomSchedule,
    }
    loaded_model = tf.keras.models.load_model(
        final_model_full_path, custom_objects=custom_objects, compile=False
    )
    print("Модель загружена.")

    # Загружаем Keras токенизатор, если он еще не загружен
    if 'tokenizer' not in locals() or tokenizer is None:
        if os.path.exists(tokenizer_full_path):
            print(f"Загрузка токенизатора из {tokenizer_full_path} для чата...")
            try:
                with open(tokenizer_full_path, "rb") as f:
                    tokenizer = pickle.load(f)
                # Обновляем глобальные переменные ID
                START_TOKEN_ID = tokenizer.word_index.get('<start>')
                END_TOKEN_ID = tokenizer.word_index.get('<end>')
                if START_TOKEN_ID is None or END_TOKEN_ID is None:
                     print("Ошибка: <start> или <end> не найдены в загруженном токенизаторе!")
                     tokenizer = None # Сбрасываем, чтобы вызвать ошибку ниже
                else:
                     print("Токенизатор Keras загружен.")
            except Exception as e:
                print(f"Ошибка загрузки токенизатора Keras: {e}")
                tokenizer = None
        else:
            print(f"Файл токенизатора не найден: {tokenizer_full_path}")
            tokenizer = None

    if tokenizer is None or START_TOKEN_ID is None or END_TOKEN_ID is None:
         print("Критическая ошибка: Токенизатор Keras недоступен или не содержит <start>/<end>. Чат не может быть запущен.")
    else:
        # Функция для генерации ответа (Адаптирована)
        def generate_response(input_text):
            # 1. Предобработка ввода пользователя (как при обучении)
            cleaned_input = preprocess_sentence(input_text)
            # print(f"  [Debug] Processed input: '{cleaned_input}'")

            # 2. Токенизация входа энкодера
            try:
                encoder_input_seq = tokenizer.texts_to_sequences([cleaned_input])[0]
            except Exception as e:
                 print(f"  [Error] Tokenization failed: {e}")
                 return "Sorry, I had trouble understanding that."

            if not encoder_input_seq:
                 # print("  [Debug] Empty sequence after tokenization.")
                 return "I couldn't understand that. Could you please rephrase?"

            # 3. Паддинг входа энкодера
            encoder_input_padded = tf.keras.preprocessing.sequence.pad_sequences(
                [encoder_input_seq], maxlen=MAX_LENGTH, padding='post', truncating='post'
            )
            encoder_input_tensor = tf.cast(encoder_input_padded, dtype=tf.int32)

            # 4. Инициализация входа декодера (начинаем с <start>)
            decoder_input_ids = [START_TOKEN_ID]

            # 5. Цикл генерации
            for i in range(MAX_LENGTH): # Ограничиваем длину ответа
                # Паддинг текущего входа декодера до MAX_LENGTH - 1 (т.к. модель ожидает вход такой длины)
                decoder_input_padded = tf.keras.preprocessing.sequence.pad_sequences(
                    [decoder_input_ids], maxlen=MAX_LENGTH-1, padding='post' # Pad до нужной длины входа декодера
                )
                decoder_input_tensor = tf.cast(decoder_input_padded, dtype=tf.int32)

                # Получаем предсказания (логиты)
                # Передаем кортеж (enc_input, dec_input)
                predictions = loaded_model((encoder_input_tensor, decoder_input_tensor), training=False)
                # predictions shape: (1, dec_seq_len, vocab_size)

                # Берем логиты для ПОСЛЕДНЕГО сгенерированного токена
                # Индекс = текущая длина dec_input - 1
                last_token_logits = predictions[:, len(decoder_input_ids)-1, :] # Shape: (1, vocab_size)

                # Применяем температуру
                scaled_logits = last_token_logits / CHAT_TEMPERATURE

                # Сэмплируем следующий токен ID
                predicted_id = tf.random.categorical(scaled_logits, num_samples=1)[0, 0].numpy()

                # Если это <end>, завершаем генерацию
                if predicted_id == END_TOKEN_ID:
                    # print(f"  [Debug] <end> token generated at step {i+1}.")
                    break

                # Добавляем предсказанный токен к входу декодера для следующего шага
                decoder_input_ids.append(predicted_id)

                 # Если достигли максимальной длины (без генерации <end>)
                if len(decoder_input_ids) >= MAX_LENGTH:
                    # print(f"  [Debug] Max length reached without <end>.")
                    break


            # 6. Декодируем результат (убираем <start>)
            output_sequence = [token_id for token_id in decoder_input_ids if token_id != START_TOKEN_ID and token_id != PAD_TOKEN_ID]

            # Используем Keras tokenizer для декодирования
            response_text = tokenizer.sequences_to_texts([output_sequence])[0]
            # Keras добавляет пробелы, уберем лишние по краям
            response_text = response_text.replace('<unk>', '...').strip() # Заменяем <unk>

            # print(f"  [Debug] Generated sequence IDs: {output_sequence}")
            # print(f"  [Debug] Decoded text: '{response_text}'")

            return response_text if response_text else "[Model generated empty response]"


        print("\nНачинаем чат! Введите 'quit' для выхода.")
        while True:
            user_input = input("Вы: ")
            if user_input.lower() == 'quit':
                break
            response = generate_response(user_input)
            print(f"Модель: {response}")

else:
    print(f"Ошибка: Файл финальной модели не найден ({final_model_full_path}). Чат не может быть запущен.")

print("\n--- Работа завершена ---")